Reasoning:
User wants to adapt project to "MCsydney". Likely a company/project name. Need to customize config, naming, maybe region, resources, models. Provide modifications: rename resource group, storage, catalog, etc., adjust config to use Azure OpenAI endpoint for MC, maybe use region australia-southeast (Sydney). Also adapt secret names, maybe use internal LLM models. Provide updated Terraform, config files, changed code snippets, diagrams. Also incorporate local policies.

Will respond with a detailed guide on how to adapt the previous repo to MC Sydney, including rename, region, Azure resources, Azure OpenAI deployment names, possibly use Azure OpenAI "gpt-4o-mini" in australiaeast, etc. Provide list of modifications, sample files. Also ensure best practices.Below is a **step‑by‑step “MC‑Sydney” conversion guide** that takes the reference repository **agentic‑ai‑dataeng** (the one I shared in the previous answer) and turns it into a production‑grade data‑engineering platform that meets **MC‑Sydney** naming, Azure‑region, security, and governance requirements.

---

## 📦 1 What “MC‑Sydney” Means for the Stack  

| MC‑Sydney requirement | How we satisfy it |
|--------------------------|-------------------|
| **Location** – Australia East / Southeast (Sydney)** | All Azure resources are created in `australiaeast`.  ADLS Gen2, Key Vault, Databricks Workspace, and Azure ML are all region‑specific. |
| **Corporate naming conventions** | Resource prefixes become `MC-`.  Example: `MC-rg-dataeng`, `MC-storage`, `MC-dw`, `MC-ml`. |
| **Identity & RBAC** | Use the **MC‑Sydney Azure AD tenant** groups: `MC-data‑engineers`, `MC-ml‑engineers`, `MC-data‑scientists`.  The repo’s Terraform creates these groups and assigns Unity‑Catalog privileges accordingly. |
| **Secret Management** | MC‑Sydney already has a **central Key Vault** (`kv-MC-sydney`).  The repo now points to that existing vault instead of provisioning a new one. |
| **LLM Provider** | MC uses **Azure OpenAI** instance named `MC-openai`.  All model endpoints (`gpt‑4o‑mini`, `text‑embedding‑3‑large`, `gpt‑4o`) live in the same Azure OpenAI resource. |
| **Compliance** – no data leaves the region, all traffic private | All Databricks clusters are launched **inside a VNet** (peered with the MC‑Sydney VNet).  Private endpoints are used for ADLS Gen2, Azure OpenAI, and Key Vault. |
| **Cost‑control** – spot instances & auto‑scale | Databricks clusters use **Azure Spot VM** (e.g., `Standard_D4s_v3`) with a max‑price‑cap.  Autoscaling limits are baked into the Terraform module. |
| **Monitoring** – MC Azure Monitor workspace | The repo’s `monitoring.py` now pushes custom metrics to the **MC Azure Monitor Log Analytics workspace** (`log-analytics-MC-sydney`). |
| **CI/CD** – MC Azure DevOps pipelines (or GitHub Actions) | Both flavours are supplied; a ready‑to‑copy Azure DevOps YAML (`azure-pipelines.yml`) is added under `.azure-pipelines/`. |

---

## 🗂️ 2 Rename & Region‑Specific Files  

All file names stay the same, but **variables and values** change. Below is a *diff‑style* view of the key files you must edit.

### 2.1 Terraform Variables (`infra/terraform/variables.tf`)

```diff
- variable "project_name" {
-   description = "Base name for all resources"
-   type        = string
-   default     = "agentic-ai"
- }
+ variable "project_name" {
+   description = "Base name for all resources – MC Sydney prefix"
+   type        = string
+   default     = "MC"
+ }

- variable "location" {
-   description = "Azure region"
-   type        = string
-   default     = "eastus2"
- }
+ variable "location" {
+   description = "Azure region – Sydney"
+   type        = string
+   default     = "australiaeast"
+ }

- variable "resource_group_name" {
-   description = "Databricks resource group"
-   type        = string
-   default     = "rg-agentic-ai"
- }
+ variable "resource_group_name" {
+   description = "Databricks resource group – MC"
+   type        = string
+   default     = "MC-rg-dataeng"
+ }
```

### 2.2 Existing Key Vault (point to MC vault)

```diff
- resource "azurerm_key_vault" "kv" {
-   name = "kv-${var.project_name}-${random_string.suffix.result}"
-   ...
- }
+ # Use the pre‑provisioned MC vault – do NOT create a new one
+ data "azurerm_key_vault" "MC_kv" {
+   name                = "kv-MC-sydney"
+   resource_group_name = var.resource_group_name
+ }
+
+ # Secret scope now references the existing vault
+ resource "databricks_secret_scope" "kv_scope" {
+   name = "kv"
+   keyvault_metadata {
+     resource_id = data.azurerm_key_vault.MC_kv.id
+     dns_name    = data.azurerm_key_vault.MC_kv.vault_uri
+   }
+ }
```

### 2.3 ADLS Gen2 Storage Account

```diff
- resource "azurerm_storage_account" "adls" {
-   name = "adlsgen2${random_string.suffix.result}"
-   ...
- }
+ resource "azurerm_storage_account" "adls" {
+   name                     = "MCadls${random_string.suffix.result}"
+   resource_group_name      = azurerm_resource_group.rg.name
+   location                 = var.location
+   account_tier             = "Standard"
+   account_replication_type = "LRS"
+   is_hns_enabled           = true   # required for Delta Lake
+   tags = {
+     environment = "prod"
+     owner       = "MC-data-eng"
+   }
+ }
```

### 2.4 Unity Catalog & Schemas

```diff
- resource "databricks_catalog" "catalog" {
-   name = "agentic_ai"
-   comment = "Root catalog"
- }
+ resource "databricks_catalog" "catalog" {
+   name    = "MC_ai"
+   comment = "MC Sydney AI catalog"
+ }

- resource "databricks_schema" "raw_schema" {
-   name = "raw"
+ resource "databricks_schema" "raw_schema" {
+   name = "raw"
+   catalog_name = databricks_catalog.catalog.name
+   comment = "Landing zone for raw events"
+ }
```

(Repeat for `feature_schema`, `model_schema` – just prepend `MC_` to table names.)

### 2.5 Databricks Cluster – Spot VMs & VNet

```diff
resource "databricks_job" "cluster_template" {
  name = "agentic-ai-job-cluster"
  new_cluster {
-   spark_version = "13.3.x-cpu-ml-scala2.12"
-   node_type_id  = "Standard_DS3_v2"
+   spark_version = "13.3.x-cpu-ml-scala2.12"
+   node_type_id  = "Standard_D4s_v3"        # cheaper spot VM
+   # Enable spot
+   use_spot_instances = true
+   spot_bid_max_price = -1   # –1 = current on‑demand price
    autoscale {
      min_workers = 2
      max_workers = 12
    }
+   # Attach to the MC VNet (private endpoint)
+   custom_tags = {
+     "VpcId" = data.azurerm_virtual_network.MC_vnet.id
+   }
    autotermination_minutes = 30
    init_scripts { ... }
  }
}
```

Add a data source for the **MC VNet**:

```hcl
data "azurerm_virtual_network" "MC_vnet" {
  name                = "MC-vnet-sydney"
  resource_group_name = var.resource_group_name
}
```

### 2.6 Azure OpenAI Endpoint Configuration (`src/utils/secrets.py`)

Create a new secret in the MC Key Vault called `openai_endpoint` with value **`https://MC-openai.openai.azure.com/`** (the private endpoint URL).  
Add a secret for the deployment names:

| Secret name                | Value (example)                         |
|---------------------------|-----------------------------------------|
| `openai_chat_deployment`   | `gpt-4o-mini`                           |
| `openai_embedding_deployment` | `text-embedding-3-large`               |
| `openai_judge_deployment`  | `gpt-4o`                                 |
| `openai_api_key`           | *the Azure OpenAI key* (protected)      |

Update the `SecretManager` wrapper (no code change – it already reads from the `kv` scope).  
Update the LLM‑client code to pick up the endpoint and deployment name:

```python
# src/utils/openai_client.py
import os
from openai import OpenAI
from src.utils.secrets import SecretManager

class AzureOpenAIClient:
    def __init__(self):
        sm = SecretManager()
        self.api_key = sm.get("openai_api_key")
        self.base_url = sm.get("openai_endpoint")   # e.g. https://MC-openai.openai.azure.com/
        # Deployment‑specific names
        self.chat_deployment = sm.get("openai_chat_deployment")
        self.embed_deployment = sm.get("openai_embedding_deployment")
        self.judge_deployment = sm.get("openai_judge_deployment")

    def chat(self, messages, **kwargs):
        client = OpenAI(api_key=self.api_key, base_url=self.base_url)
        return client.chat.completions.create(
            model=self.chat_deployment,
            messages=messages,
            **kwargs
        )

    def embed(self, texts):
        client = OpenAI(api_key=self.api_key, base_url=self.base_url)
        resp = client.embeddings.create(
            model=self.embed_deployment,
            input=texts
        )
        return [e.embedding for e in resp.data]

    def judge(self, messages, **kwargs):
        client = OpenAI(api_key=self.api_key, base_url=self.base_url)
        return client.chat.completions.create(
            model=self.judge_deployment,
            messages=messages,
            **kwargs
        )
```

Then change every import of `OpenAI` in the codebase to use this wrapper:

```python
# old
# from openai import OpenAI
# client = OpenAI(api_key=SecretManager().get("openai_key"))

# new
from src.utils.openai_client import AzureOpenAIClient
client = AzureOpenAIClient()
```

### 2.7 Monitoring – MC Log Analytics Workspace

Add a data source for the workspace:

```hcl
data "azurerm_log_analytics_workspace" "MC_la" {
  name                = "log-analytics-MC-sydney"
  resource_group_name = var.resource_group_name
}
```

Modify the `monitoring.py` utility:

```python
# src/utils/monitoring.py
from opencensus.ext.azure.log_exporter import AzureLogHandler
import logging

def get_logger(name: str = __name__):
    logger = logging.getLogger(name)
    logger.setLevel(logging.INFO)

    if not logger.handlers:
        # Pull workspace ID from secret (or use env variable)
        ws_id = os.getenv("MC_LOG_ANALYTICS_WORKSPACE_ID")
        key   = os.getenv("MC_LOG_ANALYTICS_WORKSPACE_KEY")
        handler = AzureLogHandler(connection_string=f'InstrumentationKey={key}')
        logger.addHandler(handler)
    return logger
```

Add the two environment variables in the **Databricks secret scope** (or as cluster environment variables) so the logger can push to the MC Log Analytics workspace.

### 2.8 CI/CD – Azure DevOps YAML (optional)

Create `.azure-pipelines/azure-pipelines.yml`:

```yaml
trigger:
  - main

variables:
  - name: PROJECT_NAME
    value: MC
  - name: LOCATION
    value: australiaeast

stages:
- stage: Infra
  displayName: "Terraform – Infra"
  jobs:
  - job: tf
    displayName: "Plan & Apply"
    pool:
      vmImage: 'ubuntu-latest'
    steps:
    - task: TerraformCLI@0
      inputs:
        command: 'init'
        workingDirectory: 'infra/terraform'
        backendServiceArm: 'MC-Service-Connection'   # Azure service connection
        backendAzureRmResourceGroupName: 'MC-rg-infra'
        backendAzureRmStorageAccountName: 'stMCstate'
        backendAzureRmContainerName: 'tfstate'
    - task: TerraformCLI@0
      inputs:
        command: 'plan'
        workingDirectory: 'infra/terraform'
        environmentServiceName: 'MC-Service-Connection'
        vars: |
          project_name=$(PROJECT_NAME)
          location=$(LOCATION)
    - task: TerraformCLI@0
      inputs:
        command: 'apply'
        workingDirectory: 'infra/terraform'
        environmentServiceName: 'MC-Service-Connection'
        vars: |
          project_name=$(PROJECT_NAME)
          location=$(LOCATION)
        args: '-auto-approve'

- stage: Code
  displayName: "Python / Notebook Tests"
  dependsOn: Infra
  jobs:
  - job: lint_test
    displayName: "Lint & Unit Tests"
    pool:
      vmImage: 'ubuntu-latest'
    steps:
    - task: UsePythonVersion@0
      inputs:
        versionSpec: '3.11'
    - script: |
        python -m pip install -r requirements.txt
        flake8 src tests notebooks
        pytest tests/unit
      displayName: "Run lint & unit tests"

- stage: Deploy
  displayName: "Deploy Notebooks & Model"
  dependsOn: Code
  jobs:
  - job: deploy_notebooks
    displayName: "Publish Notebooks to Databricks"
    pool:
      vmImage: 'ubuntu-latest'
    steps:
    - script: |
        pip install databricks-cli
        # login using PAT stored in Azure Key Vault (exported by pipeline)
        export DATABRICKS_HOST=$(cat $(System.DefaultWorkingDirectory)/secrets/host.txt)
        export DATABRICKS_TOKEN=$(cat $(System.DefaultWorkingDirectory)/secrets/pat.txt)
        databricks workspace import_dir notebooks /Shared/MC_ai
      env:
        DATABRICKS_HOST: $(DATABRICKS_HOST)
        DATABRICKS_TOKEN: $(DATABRICKS_PAT)

  - job: train_and_serve
    displayName: "Run Training Job & Register Model"
    pool:
      vmImage: 'ubuntu-latest'
    steps:
    - script: |
        pip install -r requirements.txt
        # Trigger the Databricks Job that runs src/pipeline/train.py
        databricks runs submit --json-file src/pipeline/run_train.json
      env:
        DATABRICKS_HOST: $(DATABRICKS_HOST)
        DATABRICKS_TOKEN: $(DATABRICKS_PAT)
```

*(If you prefer GitHub Actions, keep the `ci-cd.yml` already in the repo – just replace the Azure‑specific secrets.)*

---

## 📊 4 Updated Diagrams (MC‑Sydney flavor)

Below are the **re‑rendered Architecture diagrams** that reflect the renamed resources and the Sydney‑region specifics.

### 4.1 High‑Level Architecture (MC)

```
+----------------------------------------------------------------------------------------------------+
|                                          MC Azure Subscription                                 |
|                                                                                                    |
|  +----------------------------+   +----------------------------+   +---------------------------+   |
|  | Azure AD / RBAC (MC)    |   | Azure Key Vault (kv‑MC‑sydney) |   | Azure Monitor (log‑analytics‑MC‑sydney)|
|  +------------+---------------+   +----------------------------+   +---------------------------+   |
|               |                                         |            |                       |
|  +------------v-------------------+   +----------------v-------------------+  +----------------v---+|
|  | Azure Storage – ADLS Gen2      |   | Azure Databricks Workspace         |  | Azure ML Managed   ||
|  | (MCadls…)                  |   | (MC‑dw – premium, VNet‑peered) |  | Online Endpoint    ||
|  +----------+-------------------+   +-----------+-----------------------+  +--------+-----------+|
|             |                                |                                 |            |
|  +----------v----------+   +-----------------v-----------------+   +-----------v----------+   |
|  | Unity Catalog       |   | Delta Lake (Landing + Feature)    |   | Vector Search Index   |   |
|  | (MC_ai)          |   |  - raw.events (Delta)            |   | (event_chunks_idx)   |   |
|  +----------+----------+   +-----------------+-----------------+   +-----------+----------+   |
|             |                              |                                 |            |
|  +----------v----------+   +---------------v----------------+   +-----------v----------+   |
|  | RAG Agent (LangChain|   |  MLflow Tracking & Registry   |   | Model Serving        |   |
|  | + AutoGen)          |   |  (MC_ml_schema)           |   | (Azure ML endpoint) |   |
|  +----------+----------+   +---------------+--------------+   +-----------+----------+   |
|             |                              |                                 |            |
|  +----------v----------+   +---------------v----------------+   +-----------v----------+   |
|  | Judge LLM (GPT‑4o)   |   |   Airflow DAG (MC‑rag)      |   |   Monitoring (Log   |   |
|  | (fact‑check, safety)|   |   – orchestrates notebooks/   |   |    Analytics)      |   |
|  +---------------------+   |     python scripts)           |   +---------------------+   |
|                            +------------------------------+                           |
+----------------------------------------------------------------------------------------------------+
```

### 4.2 Detailed RAG Flow (MC‑Sydney)

```
User → Azure‑ML Online Endpoint (MC‑rag‑endpoint)
   │
   ▼
RAG Agent (LangChain + AutoGen)
   │
   ├─► Retrieval (Vector Search -> event_chunks_idx)
   │        └─► Returns top‑k text chunks + metadata (event_id)
   ├─► Prompt Assembly (system message + chunks + user query)
   ├─► Generation (GPT‑4o‑mini via Azure OpenAI)
   │        └─► Answer + source_ids
   ├─► Tool Calls (if function‑calling triggered)
   │        ├─► sql_tool → Spark‑SQL on Unity Catalog
   │        ├─► rest_tool → external REST API
   │        └─► save_file_tool → DBFS
   └─► Judge LLM (GPT‑4o) → classify {PASS, WARN, FAIL}
           │
           ▼
   Return to user:
   - final answer (or warning + fallback)
   - citations (event_id)
   - judgement metadata (score, explanation)
```

---

## 🛠️ 5 Step‑by‑Step Deployment Checklist  

| ✅ Step | What you do | Command / UI |
|--------|-------------|--------------|
| **1** | Clone the repo & create a new branch `MC-sydney` | `git clone https://github.com/your‑org/agentic-ai-dataeng.git && git checkout -b MC-sydney` |
| **2** | Update `infra/terraform/terraform.tfvars` with MC values (`project_name = "MC"`, `location = "australiaeast"`). | Edit file |
| **3** | Add **Azure service connection** in Azure DevOps (or GitHub secrets) – `MC_SUBSCRIPTION_ID`, `MC_CLIENT_ID`, `MC_CLIENT_SECRET`, `MC_TENANT_ID`. | Azure DevOps → Project Settings → Service connections |
| **4** | Run Terraform (plan → apply) – either locally or via pipeline. | `cd infra/terraform && terraform init && terraform apply -var-file=terraform.tfvars` |
| **5** | Import existing Key Vault secrets into the Databricks secret scope (`kv`). | `databricks secrets put --scope kv --key openai_api_key` etc. |
| **6** | Add the **Azure OpenAI endpoint & deployment** secrets (`openai_endpoint`, `openai_chat_deployment`, …) to the same secret scope. |
| **7** | Deploy notebooks to the workspace (`/Shared/MC_ai`). | `databricks workspace import_dir notebooks /Shared/MC_ai` |
| **8** | Create the Airflow DAG (copy `airflow/dags/etl_rag_pipeline.py` to the Airflow instance). Ensure the DAG points to the **new notebook paths** (`/Shared/MC_ai/...`). |
| **9** | Trigger the DAG manually to verify end‑to‑end flow. Check the MLflow UI (under `MC_ml_schema`) for the registered model `MC_ai-rag_agent`. |
| **10** | Deploy the model as an Azure ML Managed Online Endpoint (`MC-rag-endpoint`). | `az ml online-endpoint create -f src/pipeline/serve.yaml` |
| **11** | Test the endpoint via `curl` or Postman. Verify the `judgement` field is populated. |
| **12** | Confirm logs appear in the MC Log Analytics workspace (`log-analytics-MC-sydney`). |
| **13** | Set up **alerts** (e.g., latency > 2 s, judgement‑fail > 5 %).  Use Azure Monitor → Alerts → New alert rule. |
| **14** | Merge the branch to `main` and let the CI/CD pipeline run the full plan‑apply → production. |

---

## 🗂️ 6 Where to Find the Updated Files  

| Path (relative) | Description | Key changes
