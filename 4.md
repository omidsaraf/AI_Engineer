# RAG/LLM Addendum – Best Practices + Full Code (Company-A Sydney)

This addendum extends the main blueprint with **authoritative RAG/LLM practices**, **complete DDL**, and **ready‑to‑run notebooks, DAGs, and utilities** for a production deployment on Azure Databricks + Airflow + Azure DevOps. It is domain‑aware (Jarvis, AMDM/BCC, Pega, CMDM, SFDC) and enforces Unity Catalog governance.

---

## 1) Company-A RAG Best Practices (Authoritative)

* **Curate first**: index vetted docs and aggregated knowledge—no Bronze, no row‑level PII.
* **Domain & classification tags** on every row: `domain ∈ {jarvis, amdm, pega, cmdm, sfdc}`, `classification ∈ {public, internal, restricted}`.
* **PII controls**: redact at ingestion; use UC dynamic views (RLS/CLS); block email/phone/ID in responses.
* **Chunking**: 500–1,000 tokens; 10–20% overlap; keep headings with content.
* **Dedup/versioning**: SHA‑256 doc hash; latest by `version` + `extracted_ts`.
* **Citations required**; prefer ≥2 sources.
* **Fresh facts via tools**: query **Gold** with SQL tool for volatile info.
* **Separation of concerns**: indexing vs serving on different clusters.
* **Observability**: track hit ratio, tokens, latency, safety flags in `ep_ai.knowledge.rag_telemetry`.
* **Eval**: nightly offline metrics (faithfulness/relevancy/groundedness).
* **Rollbacks**: maintain 3 index snapshots; alias switch on deploy.
* **FinOps**: batch embeddings; cache question embeddings; prune stale chunks.

---

## 2) Knowledge Layer – Tables/Views (DDL)

```sql
CREATE VOLUME IF NOT EXISTS ep_ai.knowledge_bin COMMENT 'Binary docs for RAG';

CREATE TABLE IF NOT EXISTS ep_ai.knowledge.docs_raw (
  doc_id STRING,
  source_uri STRING,
  title STRING,
  domain STRING,
  classification STRING,
  version STRING,
  mime_type STRING,
  text STRING,
  extracted_ts TIMESTAMP
) USING DELTA;

CREATE TABLE IF NOT EXISTS ep_ai.knowledge.knowledge_cards (
  card_id STRING,
  title STRING,
  domain STRING,
  source_table STRING,
  as_of TIMESTAMP,
  classification STRING,
  text STRING
) USING DELTA;

CREATE TABLE IF NOT EXISTS ep_ai.knowledge.chunks (
  chunk_id STRING,
  parent_id STRING,
  parent_type STRING,
  domain STRING,
  classification STRING,
  seq_no INT,
  text STRING,
  embedding ARRAY<FLOAT>,
  created_ts TIMESTAMP
) USING DELTA;

CREATE OR REPLACE VIEW ep_ai.knowledge.v_chunks_public AS
SELECT * FROM ep_ai.knowledge.chunks
WHERE classification IN ('public','internal')
  AND (is_account_group_member('Company-A-analyst-readers') OR classification='public');

CREATE TABLE IF NOT EXISTS ep_ai.knowledge.rag_telemetry (
  event_ts TIMESTAMP,
  user_principal STRING,
  question STRING,
  domain_hint STRING,
  used_sql_tool BOOLEAN,
  topk INT,
  retrieved_ids ARRAY<STRING>,
  tokens_prompt INT,
  tokens_completion INT,
  latency_ms INT,
  faithfulness_score DOUBLE,
  relevancy_score DOUBLE,
  safety_flags MAP<STRING, BOOLEAN>
) USING DELTA;
```

---

## 3) Notebook – Ingest Curated Docs (ADLS) → `docs_raw`

**Path:** `notebooks/05a_rag_ingest_docs_from_adls.py`

```python
from pyspark.sql import functions as F
from pyspark.sql.functions import input_file_name

SRC_ROOT = "/mnt/knowledge/"  # jarvis/, amdm/, pega/, cmdm/, sfdc/
CAT, SCH, TBL = "ep_ai", "knowledge", "docs_raw"

df = (spark.read.format("text").load([f"{SRC_ROOT}{d}/" for d in ["jarvis","amdm","pega","cmdm","sfdc"]])
        .withColumnRenamed("value","text")
        .withColumn("source_uri", input_file_name()))

meta = (df
  .withColumn("domain", F.regexp_extract("source_uri", "/knowledge/([a-z]+)/", 1))
  .withColumn("title",  F.regexp_extract("source_uri", "/([\\w\\- ]+)__v[0-9.]+\\.\\w+$", 1))
  .withColumn("version", F.regexp_extract("source_uri", "__v([0-9.]+)\\.", 1))
  .withColumn("mime_type", F.lit("text/plain"))
  .withColumn("classification", F.lit("internal"))
  .withColumn("doc_id", F.sha2(F.col("source_uri"),256))
  .withColumn("extracted_ts", F.current_timestamp()))

(meta.select("doc_id","source_uri","title","domain","classification","version","mime_type","text","extracted_ts")
     .write.mode("append").saveAsTable(f"{CAT}.{SCH}.{TBL}"))
```

---

## 4) Notebook – Build Knowledge Cards from Gold

**Path:** `notebooks/05b_rag_build_cards_from_gold.py`

```python
from pyspark.sql import functions as F

cards = []

# Pega case metrics
case = spark.table("ep_marts.operations.fact_case")
pega_cards = (case.groupBy("case_type","status").count()
    .withColumn("title", F.concat_ws(" ", F.lit("PEGA CASE"), F.col("case_type"), F.col("status")))
    .withColumn("text", F.concat_ws(" ", F.lit("As of"), F.date_format(F.current_timestamp(),"yyyy-MM-dd"), F.lit(":"), F.col("count").cast("string")))
    .withColumn("domain", F.lit("pega")).withColumn("classification", F.lit("internal"))
    .withColumn("source_table", F.lit("ep_marts.operations.fact_case"))
    .withColumn("as_of", F.current_timestamp())
    .withColumn("card_id", F.sha2(F.concat_ws("|",F.col("title"),F.col("text")),256))
    .select("card_id","title","domain","source_table","as_of","classification","text"))

cards.append(pega_cards)

# Jarvis service status cards (template)
svc = spark.table("ep_curated.jarvis.service_silver").select("status").distinct()
jarvis_cards = (svc.withColumn("title", F.concat_ws(" ", F.lit("JARVIS SERVICE STATUS"), F.col("status")))
    .withColumn("text", F.concat_ws(" ", F.lit("Status"), F.col("status"), F.lit(": operational meaning per Company-A runbook.")))
    .withColumn("domain", F.lit("jarvis")).withColumn("classification", F.lit("public"))
    .withColumn("source_table", F.lit("ep_curated.jarvis.service_silver"))
    .withColumn("as_of", F.current_timestamp())
    .withColumn("card_id", F.sha2(F.concat_ws("|",F.col("title"),F.col("text")),256))
    .select("card_id","title","domain","source_table","as_of","classification","text"))

cards.append(jarvis_cards)

all_cards = cards[0]
for c in cards[1:]:
    all_cards = all_cards.unionByName(c, allowMissingColumns=True)

all_cards.write.mode("append").saveAsTable("ep_ai.knowledge.knowledge_cards")
```

---

## 5) Notebook – Chunk & Embed (Docs + Cards) → `chunks`

**Path:** `notebooks/05c_rag_chunk_and_embed.py`

```python
from pyspark.sql import functions as F, Window
from databricks.sdk import WorkspaceClient

CAT, SCH = "ep_ai", "knowledge"
CHUNK_SIZE = 900; OVERLAP = 150
w = WorkspaceClient()

@F.udf('array<string>')
def chunker(text: str):
    if not text: return []
    step = CHUNK_SIZE - OVERLAP
    return [text[i:i+CHUNK_SIZE] for i in range(0, max(len(text)-1,0), step)]

# Union docs + cards
src_docs = spark.table(f"{CAT}.{SCH}.docs_raw").selectExpr("doc_id as parent_id","'doc' as parent_type","domain","classification","text")
src_cards = spark.table(f"{CAT}.{SCH}.knowledge_cards").selectExpr("card_id as parent_id","'card' as parent_type","domain","classification","text")

src = src_docs.unionByName(src_cards, allowMissingColumns=True)
chunks = (src.select("parent_id","parent_type","domain","classification",F.explode(chunker("text")).alias("text"))
             .withColumn("seq_no", F.row_number().over(Window.partitionBy("parent_id").orderBy(F.monotonically_increasing_id())))
             .withColumn("chunk_id", F.sha2(F.concat_ws("|",F.col("parent_id"),F.col("seq_no"),F.col("text")),256))
             .withColumn("created_ts", F.current_timestamp()))

ENDPOINT = "Company-A-embed"  # serving endpoint name

@F.udf('array<float>')
def embed_u(text):
    # resp = w.serving_endpoints.query(name=ENDPOINT, inputs=[text])
    # return resp.get('predictions')[0]['embedding']
    return [0.0]*1536  # placeholder

(chunks.withColumn("embedding", embed_u("text"))
      .select("chunk_id","parent_id","parent_type","domain","classification","seq_no","text","embedding","created_ts")
      .write.mode("overwrite").saveAsTable(f"{CAT}.{SCH}.chunks"))
```

---

## 6) Notebook – Vector Search Index

**Path:** `notebooks/05d_rag_vector_index.py`

```python
from databricks.vector_search.client import VectorSearchClient
vsc = VectorSearchClient()
INDEX = "ep_ai.knowledge.chunks_idx"

index = vsc.get_or_create_index(
    name=INDEX,
    primary_key="chunk_id",
    embedding_dimension=1536,
    embedding_vector_column="embedding",
    source_table="ep_ai.knowledge.chunks"
)
# index.sync()  # optional
```

---

## 7) Notebook – Retrieval + SQL Tool Answering

**Path:** `notebooks/06_rag_retrieve_and_answer.py`

```python
from databricks.vector_search.client import VectorSearchClient
from databricks.sdk import WorkspaceClient
from pyspark.sql import functions as F

QUESTION = dbutils.widgets.get("question") if dbutils.widgets else "Describe Jarvis service statuses and recent Pega case counts."
DOMAIN_HINT = dbutils.widgets.get("domain") if dbutils.widgets else None
TOPK = int(dbutils.widgets.get("topk") or 5)

vsc = VectorSearchClient(); w = WorkspaceClient()
idx = vsc.get_index("ep_ai.knowledge.chunks_idx")

# Embed question
# q_emb = w.serving_endpoints.query(name="Company-A-embed", inputs=[QUESTION])["predictions"][0]["embedding"]
q_emb = [0.0]*1536

# Retrieve
filters = {"domain": DOMAIN_HINT} if DOMAIN_HINT else None
res = idx.query(embedding=q_emb, k=TOPK, filters=filters)
context = "\n\n".join([r["text"] for r in res["result"]["data"]])

# Tool-use for fresh facts
sql_bits = []
if "case" in QUESTION.lower():
    sql_bits.append("SELECT case_type, status, count(*) AS cnt FROM ep_marts.operations.fact_case GROUP BY 1,2 ORDER BY 3 DESC LIMIT 10")
if "service" in QUESTION.lower():
    sql_bits.append("SELECT status, count(*) AS cnt FROM ep_curated.jarvis.service_silver GROUP BY 1 ORDER BY 2 DESC LIMIT 10")

facts = []
for q in sql_bits:
    facts.append(spark.sql(q).toPandas().to_markdown(index=False))

prompt = f"""
System: You are an Company-A assistant. Ground answers in the provided context. Cite sources and avoid PII.
Context:\n{context}\n\nFacts (from SQL):\n{ '\n\n'.join(facts) if facts else '(none)'}\n
User: {QUESTION}
"""

# resp = w.serving_endpoints.query(name="Company-A-llm", inputs=[{"role":"user","content":prompt}])
print(prompt)

# Telemetry
spark.createDataFrame([
    (F.current_timestamp(), w.current_user.me().user_name, QUESTION, DOMAIN_HINT, len(sql_bits)>0, TOPK, [r["chunk_id"] for r in res["result"]["data"]], 0, 0, 0, None, None, None)
], "event_ts timestamp, user_principal string, question string, domain_hint string, used_sql_tool boolean, topk int, retrieved_ids array<string>, tokens_prompt int, tokens_completion int, latency_ms int, faithfulness_score double, relevancy_score double, safety_flags map<string,boolean>") \
.write.mode("append").saveAsTable("ep_ai.knowledge.rag_telemetry")
```

---

## 8) Agent SQL Tool (Domain‑aware)

**Path:** `llm_agents/tools/sql_tool.py`

```python
from dataclasses import dataclass
from pyspark.sql import SparkSession

ALLOWED_SCHEMAS = ["ep_marts","ep_curated"]  # guardrail

@dataclass
class SQLTool:
    name: str = "sql_tool"
    description: str = "Query Gold/Curated marts in Unity Catalog"

    def run(self, query: str) -> str:
        if any(p in query.lower() for p in ["insert","merge","update","delete","drop","alter"]):
            return "Blocked: write operations are not allowed."
        if not any(schema in query for schema in ALLOWED_SCHEMAS):
            return "Blocked: query must reference ep_marts/ep_curated."
        spark = SparkSession.getActiveSession()
        return spark.sql(query).limit(100).toPandas().to_markdown(index=False)
```

---

## 9) Airflow DAG – RAG Pipeline

**Path:** `orchestration/airflow/dags/rag_pipeline.py`

```python
from airflow import DAG
from airflow.utils.dates import days_ago
from airflow.providers.databricks.operators.databricks import DatabricksRunNowOperator

with DAG(
  dag_id="rag_pipeline",
  start_date=days_ago(1),
  schedule_interval=None,
  catchup=False
) as dag:

  ingest_docs = DatabricksRunNowOperator(task_id="ingest_docs", databricks_conn_id="databricks_default", job_id=1401)
  build_cards = DatabricksRunNowOperator(task_id="build_cards", databricks_conn_id="databricks_default", job_id=1402)
  chunk_embed = DatabricksRunNowOperator(task_id="chunk_embed", databricks_conn_id="databricks_default", job_id=1403)
  vs_index = DatabricksRunNowOperator(task_id="vector_index", databricks_conn_id="databricks_default", job_id=1404)

  ingest_docs >> build_cards >> chunk_embed >> vs_index
```

---

## 10) Tests – Smoke & Eval

```sql
SELECT domain, classification, COUNT(*) AS n FROM ep_ai.knowledge.chunks GROUP BY 1,2;
SELECT COUNT(*) FROM ep_ai.knowledge.v_chunks_public WHERE classification='restricted'; -- expect 0
```

**Path:** `llm_agents/evaluations/rag_metrics.py`

```python
def faithfulness(prediction: str, context: str) -> float:
    return 0.9  # placeholder

def relevancy(question: str, context: str) -> float:
    return 0.85
```

---

## 11) Operating Runbook – RAG

* Refresh index nightly 01:00 AEST or on‑demand via DAG.
* Onboard new source: add folder under `/mnt/knowledge/<domain>/`, update classification, re‑run pipeline.
* PII incident: disable `v_chunks_public`, re‑ingest with redaction, rotate index alias.
* Rollback: switch to previous index snapshot; keep 3 snapshots.

---

## 12) Hand‑off Checklist

* [ ] UC schemas/tables created (Section 2).
* [ ] Serving endpoints: `Company-A-embed`, `Company-A-llm`.
* [ ] Jobs created (Airflow job IDs 1401–1404).
* [ ] Mounts/paths for `/mnt/knowledge/*`.
* [ ] DQ checks for docs/cards; dedup; classification present.
* [ ] Telemetry dashboard wired to `rag_telemetry`.
