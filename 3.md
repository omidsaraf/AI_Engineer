# TLM Sydney – Azure Databricks Lakehouse (HLA + LLD + RAG/LLM/Agents + CI/CD + Airflow)

**Owner:** Data Engineering (TLM Sydney)
**Regions/Time Zone:** Australia East (AUS-EAST), AEST/AEDT
**Environments:** `dev` → `test` → `prod` (strict promotion, immutable artifacts)
**Repo:** Azure DevOps Repos (mono‑repo with service folders)
**Orchestrator:** Apache Airflow (env‑scoped) triggering Databricks Jobs/DLT/Serving
**Governance:** Unity Catalog (metastore + securables), Key Vault‑backed secrets, audit logs
**Security baselines:** Azure AD SSO/SCIM (least privilege), private link, VNET‑injection, ABAC/RBAC

---

## 1) Executive Summary

This blueprint defines an end‑to‑end, production‑grade Lakehouse for TLM Sydney built on Azure Databricks. It covers: ingestion (batch/stream), medallion modeling (Bronze/Silver/Gold), quality, governance, CI/CD with Azure DevOps, Airflow orchestration, and an applied GenAI stack (RAG, LLM serving, agentic tools) with operational monitoring. All code/notebooks, deployment templates, and pipelines are included with concise, “copy‑paste‑ready” examples.

---

## 2) High‑Level Architecture (HLA)

```
 ┌─────────────── Sources ───────────────┐      ┌──────────────── Platform Guardrails ────────────────┐
 │ SFDC (CSV/S3), AMDM/BCC, APIs, Kafka │      │ AAD SSO, Key Vault, Private Link, Unity Catalog,   │
 │ Files (ADLS), On‑prem DBs via ADF     │      │ Audit Logs, ABAC/RBAC, PII Masking, Lineage        │
 └───────────────────────────────────────┘      └─────────────────────────────────────────────────────┘
                │ (ADF/Batch, Streaming/Auto Loader, REST)
                ▼
        ┌──────────────┐     DQ/Schema Contracts     ┌─────────────┐
        │ Bronze (Δ)   │────────────────────────────▶│ Silver (Δ)  │
        │ Raw landing  │                            │ Clean/Conform│
        └──────────────┘◀──── Replayable Streams ────┴─────────────┘
                │                                          │
                ▼                                          ▼
        ┌──────────────┐                           ┌────────────────┐
        │  DLT/Jobs    │                           │  Gold (Δ/SQL)  │
        │  Transforms  │                           │  Marts & APIs  │
        └──────────────┘                           └────────────────┘
                 \                                       / 
                  \                                     /  BI/Apps/ML
                   ▼                                   ▼
           ┌────────────────┐                 ┌────────────────────┐
           │ Vector Search  │◀── Chunk/Embed ─│  LLM/RAG Services  │─▶ Channels (SFDC, web, bots)
           │ (UC volumes)   │                 │ Model Serving/Agents│
           └────────────────┘                 └────────────────────┘
                             ▲         ▲
                             │ Airflow │
                             └─────────┘  (DAGs trigger Jobs/DLT/Serving)
```

**Key choices**

* Storage: ADLS Gen2, Unity Catalog Volumes & Delta tables.
* Compute: Databricks Jobs, DLT for streaming/batch ETL, optimized autoscaling clusters & pools.
* Orchestration: Airflow kicks off DLT/Jobs; retries & SLAs at DAG level.
* Governance: Unity Catalog (catalog → schema → table), data lineage, table ACLs, row/column‑level security.
* GenAI: Databricks Vector Search (managed), Model Serving endpoints, Agentic tools (SQL + Retrieval + Functions).

---

## 3) Low‑Level Design (LLD) – Repository Layout

```
/ (Azure DevOps mono‑repo)
├─ infra/                         # Terraform/Bicep for Azure + Databricks (UC, WS, pools, jobs)
│  ├─ azure/                      # RG, VNet, KeyVault, Storage, Private Link
│  └─ databricks/                 # Metastore binding, UC catalogs/schemas, clusters, jobs, DLT
├─ orchestration/airflow/         # DAGs per domain; env via Variables/Connections
│  ├─ dags/
│  │  ├─ daily_ingest_dlt.py
│  │  ├─ silver_transform.py
│  │  ├─ gold_publish.py
│  │  ├─ rag_index_build.py
│  │  └─ model_ci_cd_release.py
│  └─ plugins/                    # custom operators, alerting, SLA callbacks
├─ pipelines/                     # Databricks Asset Bundles or dbx projects
│  ├─ dlt/                        # DLT pipelines (json), expectations
│  └─ jobs/                       # Jobs JSON; multi-task workflows
├─ notebooks/                     # Databricks notebooks (Py/SQL)
│  ├─ 01_bronze_autoloader.py
│  ├─ 02_silver_cleanse.py
│  ├─ 03_gold_marts.py
│  ├─ 04_quality_expectations.py
│  ├─ 05_rag_ingest_chunk_embed.py
│  ├─ 06_rag_retrieve_chain_serve.py
│  ├─ 07_streaming_kafka_to_delta.py
│  └─ 08_backfill_tools.py
├─ llm_agents/                    # RAG/Agents code & configs
│  ├─ tools/sql_tool.py
│  ├─ tools/retriever_tool.py
│  ├─ agents/TLM_assistant.py
│  ├─ prompts/
│  │  ├─ system.md
│  │  └─ templates.yaml
│  └─ evaluations/
│     ├─ offline_eval.ipynb
│     └─ rag_metrics.py
├─ quality/                       # Great Expectations / data contracts
│  ├─ expectations/
│  ├─ contracts/
│  │  └─ sfdc_contact.schema.json
│  └─ dq_rules.yaml
├─ sql/
│  ├─ create_uc_objects.sql
│  ├─ bronze_ddls.sql
│  ├─ silver_ddls.sql
│  └─ gold_ddls.sql
├─ devops/                        # Azure DevOps pipelines
│  ├─ ci.yml
│  └─ cd.yml
├─ tests/                         # pytest for PySpark/LLM utils
│  ├─ unit/
│  └─ integration/
└─ README.md
```

---

## 4) Unity Catalog (UC) objects & security (SQL)

```sql
-- Metastore, catalogs and schemas
CREATE CATALOG IF NOT EXISTS ep_raw COMMENT 'Bronze raw landing';
CREATE CATALOG IF NOT EXISTS ep_curated COMMENT 'Silver cleansed/conformed';
CREATE CATALOG IF NOT EXISTS ep_marts COMMENT 'Gold marts for BI/ML';
CREATE CATALOG IF NOT EXISTS ep_ai COMMENT 'Vector indexes, prompts, evals';

USE CATALOG ep_raw; CREATE SCHEMA IF NOT EXISTS telecom; -- example domain
USE CATALOG ep_curated; CREATE SCHEMA IF NOT EXISTS telecom;
USE CATALOG ep_marts; CREATE SCHEMA IF NOT EXISTS telecom;
USE CATALOG ep_ai; CREATE SCHEMA IF NOT EXISTS knowledge;

-- Example row/column security
CREATE OR REPLACE FUNCTION ep_curated.telecom.mask_email(email STRING)
RETURNS STRING
RETURN CASE WHEN is_account_group_member('pii_readers') THEN email ELSE regexp_replace(email, '(.{2}).+(@.*)', '$1***$2') END;

GRANT USAGE ON CATALOG ep_raw TO `data_engineering`;  
GRANT SELECT ON ALL TABLES IN SCHEMA ep_curated.telecom TO `analyst_readers`;
ALTER TABLE ep_curated.telecom.customer ADD COLUMN email_masked STRING;
```

---

## 5) Data Contracts & Schemas (JSON)

```json
{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "title": "sfdc_contact",
  "type": "object",
  "properties": {
    "contact_id": {"type": "string"},
    "first_name": {"type": "string"},
    "last_name": {"type": "string"},
    "email": {"type": "string", "format": "email"},
    "phone": {"type": "string"},
    "state": {"type": "string", "maxLength": 3},
    "postcode": {"type": "string", "maxLength": 4},
    "ingest_ts": {"type": "string", "format": "date-time"}
  },
  "required": ["contact_id", "email", "ingest_ts"],
  "additionalProperties": false
}
```

---

## 6) Bronze ingestion (Auto Loader – notebook `01_bronze_autoloader.py`)

```python
# Databricks notebook source
from pyspark.sql.functions import input_file_name, current_timestamp
from pyspark.sql.types import *

CATALOG = "ep_raw"; SCHEMA = "telecom"; TABLE = "sfdc_contact_bronze"
SRC_DIR = "/mnt/raw/sfdc/contact/"  # ADLS Gen2 path mounted or abfss://

schema = StructType([
    StructField("contact_id", StringType()),
    StructField("first_name", StringType()),
    StructField("last_name", StringType()),
    StructField("email", StringType()),
    StructField("phone", StringType()),
    StructField("state", StringType()),
    StructField("postcode", StringType()),
    StructField("_ingestion_date", DateType())
])

spark.sql(f"CREATE TABLE IF NOT EXISTS {CATALOG}.{SCHEMA}.{TABLE} USING DELTA")

df = (spark.readStream
          .format("cloudFiles")
          .option("cloudFiles.format", "csv")
          .option("cloudFiles.inferColumnTypes", "true")
          .schema(schema)
          .load(SRC_DIR)
          .withColumn("source_file", input_file_name())
          .withColumn("ingest_ts", current_timestamp()))

(df.writeStream
   .format("delta")
   .option("checkpointLocation", f"/mnt/checkpoints/{SCHEMA}/{TABLE}")
   .outputMode("append")
   .toTable(f"{CATALOG}.{SCHEMA}.{TABLE}"))
```

**Purpose:** schema‑aware, replayable ingestion; auto schema evolution can be toggled per domain.
**Ops:** checkpointed exactly‑once with Delta; Airflow handles retry/backfill tasks.

---

## 7) Silver cleanse/conform (`02_silver_cleanse.py`)

```python
from pyspark.sql import functions as F

SRC = "ep_raw.telecom.sfdc_contact_bronze"
TGT = "ep_curated.telecom.sfdc_contact_silver"

spark.sql(f"""
CREATE TABLE IF NOT EXISTS {TGT} (
  contact_id STRING, first_name STRING, last_name STRING,
  email STRING, email_domain STRING, phone STRING,
  state STRING, postcode STRING,
  ingest_ts TIMESTAMP, updated_ts TIMESTAMP
) USING DELTA
PARTITIONED BY (state)
""")

sdf = (spark.table(SRC)
        .withColumn("email", F.lower("email"))
        .withColumn("email_domain", F.regexp_extract("email", "@(.+)$", 1))
        .withColumn("updated_ts", F.current_timestamp()))

(sdf.write
    .format("delta")
    .mode("overwrite")
    .option("overwriteSchema", "true")
    .saveAsTable(TGT))
```

**Purpose:** standardize casing, derive features, hash keys if needed; partition for reads.

---

## 8) Gold marts (`03_gold_marts.py`)

```python
from pyspark.sql import functions as F

SRC = "ep_curated.telecom.sfdc_contact_silver"
TGT = "ep_marts.telecom.customer_dim"

spark.sql(f"CREATE TABLE IF NOT EXISTS {TGT} USING DELTA AS SELECT 1 AS dummy WHERE 1=0")

cur = spark.table(SRC)
cur = cur.withColumn("customer_sk", F.sha2(F.concat_ws("|", "contact_id", "email"), 256))

(cur.select("customer_sk", "contact_id", "first_name", "last_name",
            "email", "email_domain", "state", "postcode", "updated_ts")
   .write.mode("overwrite").format("delta").saveAsTable(TGT))
```

**Purpose:** star‑schema friendly tables and/or wide denormalized entities for APIs.

---

## 9) Data Quality rules (Great Expectations + DLT expectations)

`quality/dq_rules.yaml`

```yaml
checks:
  - table: ep_curated.telecom.sfdc_contact_silver
    expectations:
      - expect_column_values_to_not_be_null:
          column: contact_id
      - expect_column_values_to_match_regex:
          column: email
          regex: "^[^@\s]+@[^@\s]+\.[^@\s]+$"
      - expect_column_values_to_be_in_set:
          column: state
          value_set: [NSW, VIC, QLD, SA, WA, TAS, NT, ACT]
```

**DLT example (JSON pipeline excerpt):**

```json
{
  "name": "telecom_dlt",
  "edition": "ADVANCED",
  "clusters": [{"num_workers": 2}],
  "libraries": [],
  "development": false,
  "continuous": true,
  "edition": "ADVANCED"
}
```

---

## 10) Streaming join with watermark (Silver) – throughput aware

```python
# notebooks/07_streaming_kafka_to_delta.py
from pyspark.sql import functions as F

kafka = (spark.readStream.format("kafka")
         .option("kafka.bootstrap.servers", "<broker>")
         .option("subscribe", "loan_applications")
         .option("startingOffsets", "latest").load())

apps = (kafka.select(F.col("key").cast("string").alias("loan_id"),
                     F.col("value").cast("string").alias("payload"),
                     F.col("timestamp"))
              .withWatermark("timestamp", "2 hours"))

batch = spark.read.format("delta").table("ep_curated.telecom.sfdc_contact_silver")

joined = (apps.join(F.broadcast(batch), apps.loan_id == batch.contact_id, "inner")
               .select("loan_id", "payload", batch.email, batch.state, apps.timestamp))

(joined.writeStream
       .format("delta")
       .option("checkpointLocation", "/mnt/checkpoints/joins/loan_apps")
       .trigger(processingTime="30 seconds")
       .toTable("ep_curated.telecom.loan_apps_enriched"))
```

**Note:** watermark + trigger balances latency vs throughput; broadcasts when safe.

---

## 11) Airflow DAGs (orchestration)

`orchestration/airflow/dags/daily_ingest_dlt.py`

```python
from airflow import DAG
from airflow.utils.dates import days_ago
from airflow.providers.databricks.operators.databricks import DatabricksRunNowOperator
from airflow.models import Variable

with DAG(
    dag_id="daily_ingest_dlt",
    start_date=days_ago(1),
    schedule_interval="0 5 * * *",  # 05:00 AEST daily
    catchup=False,
    max_active_runs=1,
    default_args={"retries": 2}
) as dag:

    run_bronze = DatabricksRunNowOperator(
        task_id="run_bronze",
        databricks_conn_id="databricks_default",
        job_id=Variable.get("JOB_BRONZE_AUTLOADER_ID")
    )

    run_silver = DatabricksRunNowOperator(
        task_id="run_silver",
        databricks_conn_id="databricks_default",
        job_id=Variable.get("JOB_SILVER_CLEANSE_ID")
    )

    run_gold = DatabricksRunNowOperator(
        task_id="run_gold",
        databricks_conn_id="databricks_default",
        job_id=Variable.get("JOB_GOLD_MARTS_ID")
    )

    run_bronze >> run_silver >> run_gold
```

`orchestration/airflow/dags/rag_index_build.py`

```python
from airflow import DAG
from airflow.utils.dates import days_ago
from airflow.providers.databricks.operators.databricks import DatabricksRunNowOperator

with DAG(
    dag_id="rag_index_build",
    start_date=days_ago(1),
    schedule_interval=None,
    catchup=False
) as dag:
    build_index = DatabricksRunNowOperator(
        task_id="build_index",
        databricks_conn_id="databricks_default",
        job_id=12345  # notebooks/05_rag_ingest_chunk_embed.py
    )
```

---

## 12) Azure DevOps – CI/CD

**CI (`devops/ci.yml`)**

```yaml
trigger:
  branches: { include: [ main, dev ] }

pool: { vmImage: 'ubuntu-latest' }

variables:
  PYTHON_VERSION: '3.11'

stages:
- stage: CI
  jobs:
  - job: tests
    steps:
    - task: UsePythonVersion@0
      inputs: { versionSpec: '$(PYTHON_VERSION)' }
    - script: pip install -r requirements.txt && pip install -r requirements-dev.txt
    - script: pytest -q
    - script: flake8 notebooks llm_agents tests
```

**CD (`devops/cd.yml`)**

```yaml
trigger: none

stages:
- stage: Deploy_Dev
  jobs:
  - job: bundle
    steps:
    - script: |
        pip install databricks-sdk databricks-bundles
        databricks bundles deploy -t dev
    - script: databricks bundles run -t dev telecom_dlt

- stage: Deploy_Test
  dependsOn: Deploy_Dev
  jobs:
  - job: promote
    steps:
    - script: databricks bundles deploy -t test

- stage: Deploy_Prod
  dependsOn: Deploy_Test
  jobs:
  - job: promote
    steps:
    - script: databricks bundles deploy -t prod
```

**Bundles (`pipelines/bundles.yaml`)**

```yaml
bundle:
  name: TLM-lakehouse
resources:
  jobs:
    bronze_ingest:
      name: bronze_autoloader
      tasks:
        - task_key: bronze
          notebook_task: { notebook_path: notebooks/01_bronze_autoloader.py }
          job_cluster_key: etl_small
      job_clusters:
        - job_cluster_key: etl_small
          new_cluster:
            spark_version: 14.3.x-scala2.12
            node_type_id: Standard_DS3_v2
            num_workers: 2
  dlt_pipelines:
    telecom_dlt:
      name: telecom_dlt
      clusters: [ { num_workers: 2 } ]
      development: false
      continuous: true
targets:
  dev: { workspace: { host: https://<dev-workspace> } }
  test: { workspace: { host: https://<test-workspace> } }
  prod: { workspace: { host: https://<prod-workspace> } }
```

---

## 13) Terraform (infra)

`infra/azure/main.tf` (excerpt)

```hcl
provider "azurerm" { features {} }

resource "azurerm_resource_group" "rg" {
  name     = "rg-TLM-lakehouse"
  location = "australiaeast"
}

resource "azurerm_storage_account" "adls" {
  name                     = "TLMlakehouseadls"
  resource_group_name      = azurerm_resource_group.rg.name
  location                 = azurerm_resource_group.rg.location
  account_tier             = "Standard"
  account_replication_type = "LRS"
  is_hns_enabled           = true
}

resource "azurerm_key_vault" "kv" {
  name                = "kv-TLM-lakehouse"
  location            = azurerm_resource_group.rg.location
  resource_group_name = azurerm_resource_group.rg.name
  sku_name            = "standard"
}
```

`infra/databricks/main.tf` (excerpt)

```hcl
provider "databricks" {
  host  = var.databricks_host
  token = var.databricks_token
}

resource "databricks_metastore" "uc" {
  name          = "TLM-uc"
  storage_root  = "abfss://uc@${var.adls_name}.dfs.core.windows.net/"
  force_destroy = false
}

resource "databricks_metastore_assignment" "ws" {
  workspace_id = var.workspace_id
  metastore_id = databricks_metastore.uc.id
  default_catalog_name = "main"
}

resource "databricks_cluster" "etl_small" {
  cluster_name            = "etl-small"
  spark_version           = "14.3.x-scala2.12"
  node_type_id            = "Standard_DS3_v2"
  autotermination_minutes = 30
  num_workers             = 2
}
```

---

## 14) RAG – ingestion, chunking, embeddings, vector index

`notebooks/05_rag_ingest_chunk_embed.py`

```python
# Databricks notebook: build/update vector index from ADLS docs
import hashlib, json
from pyspark.sql import functions as F
from databricks.vector_search.client import VectorSearchClient

CAT = "ep_ai"; SCH = "knowledge"; IDX = "TLM_docs_idx"
DOC_DIR = "/mnt/knowledge/TLM_policies/"  # PDFs/HTML/MD

# 1) Load docs (simplified: text already extracted)
docs = spark.read.format("text").load(DOC_DIR).withColumnRenamed("value", "text")

# 2) Chunk
CHUNK_SIZE = 800; OVERLAP = 100

def chunk_text(text):
    chunks = []
    for i in range(0, max(len(text)-1, 0), CHUNK_SIZE-OVERLAP):
        chunks.append(text[i:i+CHUNK_SIZE])
    return chunks

udf_chunk = F.udf(lambda t: chunk_text(t) if t else [], 'array<string>')
chunks = (docs.select(F.explode(udf_chunk("text")).alias("chunk"))
              .withColumn("chunk_id", F.sha2(F.col("chunk"), 256)))

# 3) Embeddings (Databricks embeddings endpoint)
from databricks.sdk import WorkspaceClient
w = WorkspaceClient()

@F.udf('array<float>')
def embed_u(text):
    # Pseudocode: call serving endpoint "TLM-embed"
    # emb = w.serving_endpoints.query(name="TLM-embed", inputs=[text])
    # return emb.data[0].embedding
    return [0.0]*1536  # placeholder for template

vec_df = chunks.withColumn("embedding", embed_u("chunk"))

# 4) Upsert to Vector Search index
vsc = VectorSearchClient()
index = vsc.get_or_create_index(f"{CAT}.{SCH}.{IDX}", primary_key="chunk_id",
                                embedding_dimension=1536, embedding_vector_column="embedding")

(index.upsert(vec_df.select("chunk_id", "chunk", "embedding")))
```

**Notes**

* Replace placeholder UDF with the actual Model Serving endpoint call.
* Store index & docs in UC (catalog/schema).
* Maintain provenance columns: `source_uri`, `doc_id`, `version`.

---

## 15) RAG – retrieval & serving

`notebooks/06_rag_retrieve_chain_serve.py`

```python
# Simple retrieval-augmented generation chain using Vector Search + SQL tool
from databricks.vector_search.client import VectorSearchClient
from pyspark.sql import functions as F

vsc = VectorSearchClient()
index = vsc.get_index("ep_ai.knowledge.TLM_docs_idx")

QUESTION = dbutils.widgets.get("question") if dbutils.widgets else "What are TLM data retention rules?"

# 1) Embed question (call same embedding endpoint)
# question_vec = ...

# 2) Retrieve top-k
# results = index.query(embedding=question_vec, k=5)
# context = "\n\n".join([r['chunk'] for r in results])
context = "(mock) Data is retained for 7 years..."  # placeholder

# 3) Tooling: SQL lookup to gold marts (example)
# answer_sql = spark.sql("SELECT ... FROM ep_marts.telecom.customer_dim WHERE ...")

# 4) Compose final prompt
prompt = f"""
You are TLM Sydney assistant. Answer using the context below. If unsure, say you don't know.
Context:\n{context}\n\nQuestion: {QUESTION}
"""

# 5) Call LLM serving endpoint
# resp = w.serving_endpoints.query(name="TLM-llm", inputs=[{"role":"user","content":prompt}])
# print(resp)
print(prompt)
```

**Guardrails**

* Pre‑filter context by doc tags (confidentiality tiers).
* PII redaction before indexing & response.
* Prompt‑level disclaimers + refusal policies; log interactions to audit table.

---

## 16) Agentic AI (tools + orchestrator)

`llm_agents/tools/sql_tool.py`

```python
from dataclasses import dataclass
from pyspark.sql import SparkSession

@dataclass
class SQLTool:
    name:
```
