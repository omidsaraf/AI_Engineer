# MT Sydney – Azure Databricks Lakehouse (HLA + LLD + RAG/LLM/Agents + CI/CD + Airflow)

**Owner:** Data Engineering (MT Sydney)
**Regions/Time Zone:** Australia East (AUS-EAST), AEST/AEDT
**Environments:** `dev` → `test` → `prod` (strict promotion, immutable artifacts)
**Repo:** Azure DevOps Repos (mono‑repo with service folders)
**Orchestrator:** Apache Airflow (env‑scoped) triggering Databricks Jobs/DLT/Serving
**Governance:** Unity Catalog (metastore + securables), Key Vault‑backed secrets, audit logs
**Security baselines:** Azure AD SSO/SCIM (least privilege), private link, VNET‑injection, ABAC/RBAC

---

## 1) Executive Summary

This blueprint defines an end‑to‑end, production‑grade Lakehouse for MT Sydney built on Azure Databricks. It covers: ingestion (batch/stream), medallion modeling (Bronze/Silver/Gold), quality, governance, CI/CD with Azure DevOps, Airflow orchestration, and an applied GenAI stack (RAG, LLM serving, agentic tools) with operational monitoring. All code/notebooks, deployment templates, and pipelines are included with concise, “copy‑paste‑ready” examples.

---

## 2) High‑Level Architecture (HLA)

```
 ┌─────────────── Sources ───────────────┐      ┌──────────── Platform Guardrails ────────────┐
 │ SFDC, Jarvis, AMDM/BCC, Pega, CMDM,  │      │ AAD SSO, Key Vault, Private Link, Unity     │
 │ APIs, Kafka, Files(ADLS), On‑prem/ADF │      │ Catalog, Audit Logs, ABAC/RBAC, PII Masking │
 └───────────────────────────────────────┘      └──────────────────────────────────────────────┘
                │ (ADF/Batch, Auto Loader, Kafka, REST)
                ▼
        ┌──────────────┐     DQ/Contracts         ┌─────────────┐
        │ Bronze (Δ)   │─────────────────────────▶│ Silver (Δ)  │
        │ Raw Landing  │                         │ Clean/Conform│
        └──────────────┘◀── Replayable Streams ───┴─────────────┘
                │                                          │
                ▼                                          ▼
        ┌──────────────┐                           ┌────────────────┐
        │  DLT/Jobs    │                           │  Gold (Δ/SQL)  │
        │  Transforms  │                           │  Marts & APIs  │
        └──────────────┘                           └────────────────┘
                 \                                       / 
                  \                                     /  BI/Apps/ML/SFDC
                   ▼                                   ▼
           ┌────────────────┐                 ┌────────────────────┐
           │ Vector Search  │◀─ Chunk/Embed ─▶│  LLM/RAG Services  │─▶ Channels (SFDC widget, Teams bot)
           │ (UC volumes)   │                 │ Model Serving/Agents│
           └────────────────┘                 └────────────────────┘
                             ▲         ▲
                             │ Airflow │  (DAGs trigger Jobs/DLT/Serving)
                             └─────────┘
```

**Source→Domain mapping (MT):**

* **Jarvis** → Services/Accounts, usage, operational events.
* **AMDM/BCC** → Address master & geo attribs.
* **Pega** → Case management (interactions, complaints, journeys).
* **CMDM** → Customer master and survivorship rules.
* **SFDC** → Sales/Service cloud extracts and outbound updates.

**Key choices**

* Storage: ADLS Gen2, Unity Catalog Volumes & Delta tables.
* Compute: Databricks Jobs, DLT for streaming/batch ETL, optimized autoscaling clusters & pools.
* Orchestration: Airflow kicks off DLT/Jobs; retries & SLAs at DAG level.
* Governance: Unity Catalog (catalog → schema → table), data lineage, table ACLs, row/column‑level security.
* GenAI: Databricks Vector Search (managed), Model Serving endpoints, Agentic tools (SQL + Retrieval + Functions).

---

## 3) Low‑Level Design (LLD) – Repository Layout

```
/ (Azure DevOps mono‑repo)
├─ infra/
│  ├─ azure/
│  └─ databricks/
├─ orchestration/airflow/
│  ├─ dags/
│  │  ├─ daily_ingest_dlt.py
│  │  ├─ domain_ingest_daily.py          # Jarvis/AMDM/Pega/CMDM
│  │  ├─ silver_transform.py
│  │  ├─ gold_publish.py
│  │  ├─ rag_index_build.py
│  │  └─ model_ci_cd_release.py
│  └─ plugins/
├─ domains/
│  ├─ jarvis/
│  │  ├─ contracts/jarvis_service.schema.json
│  │  ├─ notebooks/bronze_autoloader_jarvis.py
│  │  ├─ notebooks/silver_service_cleanse.py
│  │  └─ notebooks/gold_service_marts.py
│  ├─ amdm/
│  │  ├─ contracts/amdm_address.schema.json
│  │  ├─ notebooks/bronze_autoloader_amdm.py
│  │  ├─ notebooks/silver_address_standardize.py
│  │  └─ notebooks/gold_address_dim.py
│  ├─ pega/
│  │  ├─ contracts/pega_case.schema.json
│  │  ├─ notebooks/bronze_autoloader_pega.py
│  │  ├─ notebooks/silver_case_flatten.py
│  │  └─ notebooks/gold_case_metrics.py
│  └─ cmdm/
│     ├─ contracts/cmdm_customer.schema.json
│     ├─ notebooks/bronze_autoloader_cmdm.py
│     ├─ notebooks/silver_master_survivorship.py
│     └─ notebooks/gold_customer_dim_unified.py
├─ pipelines/
│  ├─ dlt/
│  └─ jobs/
├─ notebooks/
│  ├─ 01_bronze_autoloader.py            # generic template
│  ├─ 02_silver_cleanse.py
│  ├─ 03_gold_marts.py
│  ├─ 04_quality_expectations.py
│  ├─ 05_rag_ingest_chunk_embed.py
│  ├─ 06_rag_retrieve_chain_serve.py
│  ├─ 07_streaming_kafka_to_delta.py
│  └─ 08_backfill_tools.py
├─ llm_agents/
├─ quality/
├─ sql/
├─ devops/
├─ tests/
└─ README.md
```

---

## 4) Unity Catalog (UC) objects & security (SQL)

```sql
-- Catalogs
CREATE CATALOG IF NOT EXISTS ep_raw COMMENT 'Bronze raw landing';
CREATE CATALOG IF NOT EXISTS ep_curated COMMENT 'Silver cleansed/conformed';
CREATE CATALOG IF NOT EXISTS ep_marts COMMENT 'Gold marts for BI/ML';
CREATE CATALOG IF NOT EXISTS ep_ai COMMENT 'Vector indexes, prompts, evals';

-- Schemas per domain
-- Bronze/Silver/Gold mirror the same domain names for clarity
USE CATALOG ep_raw;    CREATE SCHEMA IF NOT EXISTS jarvis; CREATE SCHEMA IF NOT EXISTS amdm; CREATE SCHEMA IF NOT EXISTS pega; CREATE SCHEMA IF NOT EXISTS cmdm; CREATE SCHEMA IF NOT EXISTS sfdc;
USE CATALOG ep_curated;CREATE SCHEMA IF NOT EXISTS jarvis; CREATE SCHEMA IF NOT EXISTS amdm; CREATE SCHEMA IF NOT EXISTS pega; CREATE SCHEMA IF NOT EXISTS cmdm; CREATE SCHEMA IF NOT EXISTS sfdc;
USE CATALOG ep_marts;  CREATE SCHEMA IF NOT EXISTS telecom; CREATE SCHEMA IF NOT EXISTS customer; CREATE SCHEMA IF NOT EXISTS operations; -- logical marts
USE CATALOG ep_ai;     CREATE SCHEMA IF NOT EXISTS knowledge;

-- Masking example
CREATE OR REPLACE FUNCTION ep_curated.customer.mask_email(email STRING)
RETURNS STRING
RETURN CASE WHEN is_account_group_member('pii_readers') THEN email ELSE regexp_replace(email, '(.{2}).+(@.*)', '$1***$2') END;

-- Grants
GRANT USAGE ON CATALOG ep_raw TO `data_engineering`;
GRANT SELECT ON ALL TABLES IN SCHEMA ep_curated.customer TO `analyst_readers`;
```

---

## 5) Data Contracts & Schemas (JSON)

```json
{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "title": "sfdc_contact",
  "type": "object",
  "properties": {
    "contact_id": {"type": "string"},
    "first_name": {"type": "string"},
    "last_name": {"type": "string"},
    "email": {"type": "string", "format": "email"},
    "phone": {"type": "string"},
    "state": {"type": "string", "maxLength": 3},
    "postcode": {"type": "string", "maxLength": 4},
    "ingest_ts": {"type": "string", "format": "date-time"}
  },
  "required": ["contact_id", "email", "ingest_ts"],
  "additionalProperties": false
}
```

---

## 6) Bronze ingestion (Auto Loader – notebook `01_bronze_autoloader.py`)

```python
# Parameterized template supporting Jarvis/AMDM/Pega/CMDM/SFDC
from pyspark.sql.functions import input_file_name, current_timestamp
from pyspark.sql.types import *

SYSTEM = dbutils.widgets.get("system") if dbutils.widgets else "sfdc"  # jarvis|amdm|pega|cmdm|sfdc
CATALOG = "ep_raw"; SCHEMA = SYSTEM; TABLE = f"{SYSTEM}_bronze"
SRC_DIR = f"/mnt/raw/{SYSTEM}/"  # per‑domain landing

schemas = {
  "amdm": StructType([
      StructField("amdm_addr_id", StringType()),
      StructField("addr_line_1_txt", StringType()),
      StructField("addr_line_2_txt", StringType()),
      StructField("sbrb_nm", StringType()),
      StructField("state", StringType()),
      StructField("post_cd", StringType())
  ]),
  "cmdm": StructType([
      StructField("customer_id", StringType()),
      StructField("first_name", StringType()),
      StructField("last_name", StringType()),
      StructField("email", StringType()),
      StructField("phone", StringType())
  ]),
  "pega": StructType([
      StructField("case_id", StringType()),
      StructField("case_type", StringType()),
      StructField("status", StringType()),
      StructField("created_ts", TimestampType())
  ]),
  "jarvis": StructType([
      StructField("service_id", StringType()),
      StructField("account_id", StringType()),
      StructField("status", StringType()),
      StructField("start_dt", DateType())
  ])
}

schema = schemas.get(SYSTEM, None)
if schema is None:
    raise ValueError(f"Unknown system: {SYSTEM}")

spark.sql(f"CREATE TABLE IF NOT EXISTS {CATALOG}.{SCHEMA}.{TABLE} USING DELTA")

df = (spark.readStream.format("cloudFiles").option("cloudFiles.format","csv").schema(schema)
        .load(SRC_DIR).withColumn("source_file", input_file_name()).withColumn("ingest_ts", current_timestamp()))

(df.writeStream.format("delta").option("checkpointLocation", f"/mnt/checkpoints/{SCHEMA}/{TABLE}").outputMode("append")
   .toTable(f"{CATALOG}.{SCHEMA}.{TABLE}"))
```

**Purpose:** Single template drives all domain ingestions via a `system` param; Airflow passes the value per task.

---

## 7) Silver cleanse/conform (`02_silver_cleanse.py`)

**AMDM – address standardization**

```python
from pyspark.sql import functions as F
SRC = "ep_raw.amdm.amdm_bronze"; TGT = "ep_curated.amdm.address_silver"
spark.sql(f"CREATE TABLE IF NOT EXISTS {TGT} USING DELTA")
addr = (spark.table(SRC)
          .withColumn("state", F.upper("state"))
          .withColumn("postcode", F.lpad("post_cd", 4, '0'))
          .withColumn("full_address", F.concat_ws(' ', "addr_line_1_txt","addr_line_2_txt","sbrb_nm","state","postcode"))
          .withColumn("updated_ts", F.current_timestamp()))
addr.write.mode("overwrite").saveAsTable(TGT)
```

**CMDM – survivorship**

```python
SRC = "ep_raw.cmdm.cmdm_bronze"; TGT = "ep_curated.cmdm.customer_silver"
spark.sql(f"CREATE TABLE IF NOT EXISTS {TGT} USING DELTA")
cmdm = spark.table(SRC).dropDuplicates(["customer_id"])  # proxy for survivorship
cmdm.write.mode("overwrite").saveAsTable(TGT)
```

**Jarvis – services**

```python
SRC = "ep_raw.jarvis.jarvis_bronze"; TGT = "ep_curated.jarvis.service_silver"
spark.sql(f"CREATE TABLE IF NOT EXISTS {TGT} USING DELTA")
svc = spark.table(SRC)
svc.write.mode("overwrite").saveAsTable(TGT)
```

**Pega – cases**

```python
SRC = "ep_raw.pega.pega_bronze"; TGT = "ep_curated.pega.case_silver"
spark.sql(f"CREATE TABLE IF NOT EXISTS {TGT} USING DELTA")
case = spark.table(SRC).withColumn("status", F.upper("status"))
case.write.mode("overwrite").saveAsTable(TGT)
```

---

## 8) Gold marts (`03_gold_marts.py`)

**Unified Customer (CMDM‑anchored) with Jarvis services and AMDM address**

```python
from pyspark.sql import functions as F
cust = spark.table("ep_curated.cmdm.customer_silver").alias("c")
addr = spark.table("ep_curated.amdm.address_silver").alias("a")
svc  = spark.table("ep_curated.jarvis.service_silver").alias("s")

unified = (cust
  .join(addr, F.col("c.customer_id") == F.col("a.amdm_addr_id"), "left")  # replace with real keys/mappings
  .join(svc, F.col("c.customer_id") == F.col("s.account_id"), "left")
  .withColumn("customer_sk", F.sha2(F.concat_ws("|","c.customer_id","c.email"),256)))

TGT = "ep_marts.customer.dim_customer"
spark.sql(f"CREATE TABLE IF NOT EXISTS {TGT} USING DELTA AS SELECT 1 AS dummy WHERE 1=0")
unified.select("customer_sk","c.*","a.full_address","s.status").write.mode("overwrite").saveAsTable(TGT)
```

**Pega Case Metrics**

```python
case = spark.table("ep_curated.pega.case_silver")
metrics = (case.groupBy("case_type","status").count().withColumn("as_of", F.current_timestamp()))
metrics.write.mode("overwrite").saveAsTable("ep_marts.operations.case_metrics")
```

---

## 9) Data Quality rules (Great Expectations + DLT expectations)

`quality/dq_rules.yaml`

```yaml
checks:
  - table: ep_curated.amdm.address_silver
    expectations:
      - expect_column_values_to_not_be_null: { column: amdm_addr_id }
      - expect_column_values_to_match_regex: { column: postcode, regex: "^\d{4}$" }
      - expect_column_values_to_be_in_set: { column: state, value_set: [NSW,VIC,QLD,SA,WA,TAS,NT,ACT] }
  - table: ep_curated.cmdm.customer_silver
    expectations:
      - expect_column_values_to_not_be_null: { column: customer_id }
      - expect_column_values_to_match_regex: { column: email, regex: "^[^@\s]+@[^@\s]+\.[^@\s]+$" }
  - table: ep_curated.pega.case_silver
    expectations:
      - expect_column_values_to_be_in_set: { column: status, value_set: [OPEN,IN_PROGRESS,RESOLVED,CLOSED] }
```

---

## 10) Streaming join with watermark (Silver) – throughput aware

```python
# notebooks/07_streaming_kafka_to_delta.py
from pyspark.sql import functions as F

kafka = (spark.readStream.format("kafka")
         .option("kafka.bootstrap.servers", "<broker>")
         .option("subscribe", "loan_applications")
         .option("startingOffsets", "latest").load())

apps = (kafka.select(F.col("key").cast("string").alias("loan_id"),
                     F.col("value").cast("string").alias("payload"),
                     F.col("timestamp"))
              .withWatermark("timestamp", "2 hours"))

batch = spark.read.format("delta").table("ep_curated.telecom.sfdc_contact_silver")

joined = (apps.join(F.broadcast(batch), apps.loan_id == batch.contact_id, "inner")
               .select("loan_id", "payload", batch.email, batch.state, apps.timestamp))

(joined.writeStream
       .format("delta")
       .option("checkpointLocation", "/mnt/checkpoints/joins/loan_apps")
       .trigger(processingTime="30 seconds")
       .toTable("ep_curated.telecom.loan_apps_enriched"))
```

**Note:** watermark + trigger balances latency vs throughput; broadcasts when safe.

---

## 11) Airflow DAGs (orchestration)

`orchestration/airflow/dags/domain_ingest_daily.py`

```python
from airflow import DAG
from airflow.utils.dates import days_ago
from airflow.providers.databricks.operators.databricks import DatabricksRunNowOperator

with DAG(
    dag_id="domain_ingest_daily",
    start_date=days_ago(1),
    schedule_interval="0 4 * * *",  # 04:00 AEST
    catchup=False,
    max_active_runs=1
) as dag:

    jarvis = DatabricksRunNowOperator(task_id="ingest_jarvis", databricks_conn_id="databricks_default", job_id=1111)
    amdm   = DatabricksRunNowOperator(task_id="ingest_amdm",   databricks_conn_id="databricks_default", job_id=2222)
    pega   = DatabricksRunNowOperator(task_id="ingest_pega",   databricks_conn_id="databricks_default", job_id=3333)
    cmdm   = DatabricksRunNowOperator(task_id="ingest_cmdm",   databricks_conn_id="databricks_default", job_id=4444)

    jarvis >> amdm >> pega >> cmdm
```

*(Existing DAGs for silver/gold/RAG remain as‑is.)*

---

## 12) Azure DevOps – CI/CD

**CI (`devops/ci.yml`)**

```yaml
trigger:
  branches: { include: [ main, dev ] }

pool: { vmImage: 'ubuntu-latest' }

variables:
  PYTHON_VERSION: '3.11'

stages:
- stage: CI
  jobs:
  - job: tests
    steps:
    - task: UsePythonVersion@0
      inputs: { versionSpec: '$(PYTHON_VERSION)' }
    - script: pip install -r requirements.txt && pip install -r requirements-dev.txt
    - script: pytest -q
    - script: flake8 notebooks llm_agents tests
```

**CD (`devops/cd.yml`)**

```yaml
trigger: none

stages:
- stage: Deploy_Dev
  jobs:
  - job: bundle
    steps:
    - script: |
        pip install databricks-sdk databricks-bundles
        databricks bundles deploy -t dev
    - script: databricks bundles run -t dev telecom_dlt

- stage: Deploy_Test
  dependsOn: Deploy_Dev
  jobs:
  - job: promote
    steps:
    - script: databricks bundles deploy -t test

- stage: Deploy_Prod
  dependsOn: Deploy_Test
  jobs:
  - job: promote
    steps:
    - script: databricks bundles deploy -t prod
```

**Bundles (`pipelines/bundles.yaml`)**

```yaml
bundle:
  name: MT-lakehouse
resources:
  jobs:
    bronze_ingest:
      name: bronze_autoloader
      tasks:
        - task_key: bronze
          notebook_task: { notebook_path: notebooks/01_bronze_autoloader.py }
          job_cluster_key: etl_small
      job_clusters:
        - job_cluster_key: etl_small
          new_cluster:
            spark_version: 14.3.x-scala2.12
            node_type_id: Standard_DS3_v2
            num_workers: 2
  dlt_pipelines:
    telecom_dlt:
      name: telecom_dlt
      clusters: [ { num_workers: 2 } ]
      development: false
      continuous: true
targets:
  dev: { workspace: { host: https://<dev-workspace> } }
  test: { workspace: { host: https://<test-workspace> } }
  prod: { workspace: { host: https://<prod-workspace> } }
```

---

## 13) Terraform (infra)

`infra/azure/main.tf` (excerpt)

```hcl
provider "azurerm" { features {} }

resource "azurerm_resource_group" "rg" {
  name     = "rg-MT-lakehouse"
  location = "australiaeast"
}

resource "azurerm_storage_account" "adls" {
  name                     = "MTlakehouseadls"
  resource_group_name      = azurerm_resource_group.rg.name
  location                 = azurerm_resource_group.rg.location
  account_tier             = "Standard"
  account_replication_type = "LRS"
  is_hns_enabled           = true
}

resource "azurerm_key_vault" "kv" {
  name                = "kv-MT-lakehouse"
  location            = azurerm_resource_group.rg.location
  resource_group_name = azurerm_resource_group.rg.name
  sku_name            = "standard"
}
```

`infra/databricks/main.tf` (excerpt)

```hcl
provider "databricks" {
  host  = var.databricks_host
  token = var.databricks_token
}

resource "databricks_metastore" "uc" {
  name          = "MT-uc"
  storage_root  = "abfss://uc@${var.adls_name}.dfs.core.windows.net/"
  force_destroy = false
}

resource "databricks_metastore_assignment" "ws" {
  workspace_id = var.workspace_id
  metastore_id = databricks_metastore.uc.id
  default_catalog_name = "main"
}

resource "databricks_cluster" "etl_small" {
  cluster_name            = "etl-small"
  spark_version           = "14.3.x-scala2.12"
  node_type_id            = "Standard_DS3_v2"
  autotermination_minutes = 30
  num_workers             = 2
}
```

---

## 14) RAG – ingestion, chunking, embeddings, vector index

Add domain‑tagged ingestion so answers can be source‑aware.

```python
# notebooks/05_rag_ingest_chunk_embed.py (delta)
from pyspark.sql import functions as F
from databricks.vector_search.client import VectorSearchClient

CAT, SCH, IDX = "ep_ai", "knowledge", "MT_docs_idx"

DOC_DIRS = [
  ("jarvis", "/mnt/knowledge/jarvis/"),
  ("amdm",   "/mnt/knowledge/amdm/"),
  ("pega",   "/mnt/knowledge/pega/"),
  ("cmdm",   "/mnt/knowledge/cmdm/")
]

rows = []
for tag, path in DOC_DIRS:
    df = spark.read.format("text").load(path).withColumnRenamed("value","text")
    df = df.withColumn("domain", F.lit(tag))
    rows.append(df)

docs = rows[0]
for r in rows[1:]:
    docs = docs.unionByName(r, allowMissingColumns=True)

# ... chunk, embed as before, plus include 'domain' and 'source_uri'
```

Index schema adds `domain` for filtering and attribution.

---

## 15) RAG – retrieval & serving

`notebooks/06_rag_retrieve_chain_serve.py`

```python
# Simple retrieval-augmented generation chain using Vector Search + SQL tool
from databricks.vector_search.client import VectorSearchClient
from pyspark.sql import functions as F

vsc = VectorSearchClient()
index = vsc.get_index("ep_ai.knowledge.MT_docs_idx")

QUESTION = dbutils.widgets.get("question") if dbutils.widgets else "What are MT data retention rules?"

# 1) Embed question (call same embedding endpoint)
# question_vec = ...

# 2) Retrieve top-k
# results = index.query(embedding=question_vec, k=5)
# context = "\n\n".join([r['chunk'] for r in results])
context = "(mock) Data is retained for 7 years..."  # placeholder

# 3) Tooling: SQL lookup to gold marts (example)
# answer_sql = spark.sql("SELECT ... FROM ep_marts.telecom.customer_dim WHERE ...")

# 4) Compose final prompt
prompt = f"""
You are MT Sydney assistant. Answer using the context below. If unsure, say you don't know.
Context:\n{context}\n\nQuestion: {QUESTION}
"""

# 5) Call LLM serving endpoint
# resp = w.serving_endpoints.query(name="MT-llm", inputs=[{"role":"user","content":prompt}])
# print(resp)
print(prompt)
```

**Guardrails**

* Pre‑filter context by doc tags (confidentiality tiers).
* PII redaction before indexing & response.
* Prompt‑level disclaimers + refusal policies; log interactions to audit table.

---

## 16) Agentic AI (tools + orchestrator)

Augment tools to be domain‑aware.

`llm_agents/tools/retriever_tool.py`

```python
from dataclasses import dataclass

@dataclass
class RetrieverTool:
    name: str = "retriever_tool"
    description: str = "Retrieve domain‑tagged knowledge chunks"

    def run(self, question: str, domain: str = None) -> str:
        # Call vector index with optional domain filter
        # return joined passages
        return f"(mock passages for domain={domain})"
```

Routing example: if user mentions *case* → prefer **pega**; *address* → **amdm**; *service* → **jarvis**; *identity* → **cmdm**.

---

## 17) Testing strategy

* **PySpark unit tests:** schema assertions, UDFs, transformations (`tests/unit`).
* **Data QA:** Great Expectations suites on Bronze→Silver and Silver→Gold.
* **E2E tests:** DAG dry‑runs; DLT expectations fail‑fast; backfill simulations.
* **LLM eval:** offline RAG metrics (faithfulness, answer relevancy), guardrail tests for safety.

---

## 18) Observability & Ops

* **Lakehouse Monitoring:** table freshness, volume drift, schema drift alerts.
* **Airflow SLAs:** task duration and success rate; Opsgenie/PagerDuty alerts.
* **Serving metrics:** latency, 95p response time, token usage, retrieval hit ratio.
* **Cost:** job tags `ENVM`, `COST_CENTER`, photon on SQL/ETL where beneficial; auto‑stop.

---

## 19) Promotion flow & branching

* Trunk‑based: `main` → feature branches; PRs with checks (tests + lint + security scan).
* Environment promotion via Bundles targets (`dev`, `test`, `prod`) with approvals & change tickets.
* Artifact immutability: bundle SHA + job versions.

---

## 20) Runbook (abridged)

* **Backfill:** parameterized notebook `08_backfill_tools.py` by date range; Airflow ad‑hoc DAG run.
* **Hotfix:** cherry‑pick to `main`, redeploy via `Deploy_Dev` → `Test` → `Prod`.
* **Schema change:** update contract → DLT expectations → versioned migration script.
* **Key rotation:** managed in Key Vault; restart affected jobs.

---

## 21) Acceptance KPIs

* DQ pass rate ≥ 99%; SLA meets 95p freshness < 2h for daily feeds, < 5m for streams.
* GenAI: grounded responses with factuality score ≥ 0.8; retrieval hit ratio ≥ 0.9 for known queries.
* Change failure rate < 10%; MTTR < 30m.

---

## 23) Domain notes & key joins (MT)

* **CMDM ↔ Jarvis**: `cmdm.customer_id` ⇄ `jarvis.account_id` (customer→accounts/services).
* **CMDM ↔ AMDM**: address mastered in AMDM/BCC; attach via `amdm_addr_id` or standardized address hash.
* **CMDM ↔ SFDC**: identity & contact sync via SFDC Contact/Account IDs.
* **Pega ↔ CMDM/Jarvis**: `case.party_id/customer_id` and `service_id` for interaction lineage.

> Replace surrogate keys with exact production keys from Mandos/UDP mapping once confirmed.

---

## 24) MT Best Practices (Security, Governance, Engineering)

### 24.1 Security & Networking

* Private-by-default: VNET injection for Databricks workspaces, no public IP on clusters; storage firewall and Private Endpoints for ADLS, Key Vault, ACR, Event Hub/Kafka, SQL.
* Identity: Azure AD SSO and SCIM, groups per role: `MT-dpm-data-eng`, `MT-analyst-readers`, `MT-ml-serving`, `MT-ai-governance`.
* Secrets: Azure Key Vault bound to Databricks secret scopes; secret scanning in CI.
* Audit: Workspace audit logs to ADLS `logs/audit/` and SIEM with long retention.
* Data classification: PII tagging in Unity Catalog; column masks; dynamic views for RLS by LOB/region/role.

**Cluster Policy (enforced)** `infra/databricks/policies/etl_restricted.json`

```json
{
  "spark_version": {"type":"fixed", "value":"14.3.x-scala2.12"},
  "autotermination_minutes": {"type":"range","minValue":15,"maxValue":120,"defaultValue":30},
  "num_workers": {"type":"range","minValue":2,"maxValue":16,"defaultValue":4},
  "enable_elastic_disk": {"type":"fixed","value": true},
  "custom_tags.ENVM": {"type":"fixed","value":"${env}"},
  "custom_tags.UDP_BU_DB_COST": {"type":"fixed","value":"GRP_IT-DPM"},
  "data_security_mode": {"type":"fixed","value":"SINGLE_USER"},
  "azure_attributes.availability": {"type":"fixed","value":"ON_DEMAND_AZURE"}
}
```

### 24.2 Governance & Data Contracts

* Unity Catalog-first: catalogs per zone; schemas per domain; grants via AAD groups only.
* Contracts: JSON Schema and example payloads stored in repo and UC Volume; change control via PR and version bump with consumer notification.
* DQ as gate: DLT expectations and GE suites act as circuit-breakers; invalid rows redirected to Quarantine tables.

**Dynamic View (RLS)**

```sql
CREATE OR REPLACE VIEW ep_curated.customer.v_customer_secure AS
SELECT *,
  CASE WHEN is_account_group_member('MT-analyst-readers') THEN email ELSE ep_curated.customer.mask_email(email) END AS email_safe
FROM ep_curated.cmdm.customer_silver;
```

### 24.3 Delta Lake Engineering

* Idempotent upserts: deterministic natural keys and window de-duplication in Silver.
* SCD2 where required (CMDM): columns `effective_from`, `effective_to`, `is_current`.
* Performance: OPTIMIZE with ZORDER on selective columns; nightly small-file compaction jobs.
* Retention: VACUUM 7 days in dev/test and 30+ days in prod; table properties set for log retention.

**Silver De-duplication (idempotent)**

```sql
CREATE OR REPLACE TABLE ep_curated.cmdm.customer_silver USING DELTA AS
SELECT * FROM (
  SELECT *,
    ROW_NUMBER() OVER (PARTITION BY customer_id ORDER BY ingest_ts DESC) AS rn
  FROM ep_raw.cmdm.cmdm_bronze
) WHERE rn = 1;
```

**SCD2 Merge (CMDM to Gold DIM)**

```sql
MERGE INTO ep_marts.customer.dim_customer_scd2 AS tgt
USING (
  SELECT c.*, current_timestamp() AS _ts
  FROM ep_curated.cmdm.customer_silver c
) src
ON tgt.customer_id = src.customer_id AND tgt.is_current = true
WHEN MATCHED AND (
    sha2(concat_ws('|',src.first_name,src.last_name,src.email,src.phone),256) 
  <> sha2(concat_ws('|',tgt.first_name,tgt.last_name,tgt.email,tgt.phone),256)
) THEN UPDATE SET
  tgt.effective_to = src._ts,
  tgt.is_current = false
WHEN NOT MATCHED THEN INSERT (
  customer_id, first_name, last_name, email, phone,
  effective_from, effective_to, is_current
) VALUES (
  src.customer_id, src.first_name, src.last_name, src.email, src.phone,
  src._ts, TIMESTAMP'9999-12-31 00:00:00', true
);
```

**Compaction Job (SQL)**

```sql
OPTIMIZE ep_curated.jarvis.service_silver ZORDER BY (account_id);
VACUUM ep_curated.jarvis.service_silver RETAIN 240 HOURS;
```

**Quarantine Pattern**

```sql
CREATE TABLE IF NOT EXISTS ep_raw.cmdm.cmdm_quarantine (payload STRING, reason STRING, ingest_ts TIMESTAMP) USING DELTA;
```

### 24.4 Observability & FinOps

* Tags everywhere: `ENVM`, `COST_CENTER`, `SYSTEM`, `DATA_DOMAIN` on clusters and jobs.
* SLAs: Airflow task-level SLAs; alert to `dpm_udp_devops@MT.com.au` and Teams webhook.
* Metrics: Lakehouse Monitoring plus custom tables for row counts, freshness, and DQ pass rate.

**Airflow Callbacks** `orchestration/airflow/plugins/callbacks.py`

```python
from airflow.utils.email import send_email

def on_failure(context):
    send_email(to=['dpm_udp_devops@MT.com.au'],
               subject=f"[ALERT] {context['task_instance_key_str']}",
               html_content=str(context))
```

### 24.5 CI/CD & Release Management (Azure DevOps)

* Branching: trunk-based; short-lived feature branches; protected `main`.
* Controls: unit and integration tests, security scan, data contract validation in CI.
* Promotion: dev → test → prod via Bundles with gated approvals and CAB ticket; immutable artifact SHA.
* Backfills: parameterized pipelines with approved runbooks.

**Contract Validation (CI step)**

```bash
python tools/validate_contracts.py --schemas domains/**/contracts/*.json --samples samples/**/*.json
```

### 24.6 RAG/LLM Guardrails

* Document hygiene: remove PII where not needed; attach `classification` and `retention` tags.
* Retrieval filters: domain and classification allow-lists; cap on top-k; de-duplicate passages; include source citations.
* Prompt safety: system prompt with refusals; simple pre-answer redaction function.
* Telemetry: log question, retrieved doc IDs, answer size, latency, safety flags.

**Simple Redaction Stub**

```python
from pyspark.sql import functions as F
@F.udf('string')
def redact(s):
    if s is None: return None
    return s.replace('@', '[at]')
```

### 24.7 Key Management & Secrets

* Key Vault for all connectors (SFDC, Kafka, SQL) with quarterly rotation.
* Secret scope creation tracked in infrastructure-as-code under `infra/databricks`.

### 24.8 Backup and DR

* ADLS GRS for critical containers and a documented cross-region recovery runbook.
* Vector index snapshots exported to UC Volume weekly.
* DLT checkpoint backups retained at least 7 days.

---

## 25) MT Domain Playbooks (Jarvis, AMDM, Pega, CMDM)

**Jarvis → Services**

* Natural key: `service_id`; business key join to CMDM via `account_id`.
* Silver rules: valid status set; enforce `start_dt <= current_date()`; quarantine future-dated rows.

**AMDM/BCC → Address**

* Standardize fields; derive geo attributes where available; preserve GCC and mesh block attributes.
* Join to CMDM via `amdm_addr_id` or address hash (street + suburb + state + postcode).

**Pega → Cases and Interactions**

* Normalize `case_type` and `status`; link to CMDM via `customer_id` and to Jarvis via `service_id` when present.
* Gold metrics: volumes, SLA breach percentage, average handle time, reopen rate.

**CMDM → Customer**

* Survivorship proxy in Silver; SCD2 in Gold; email and phone hashing for identity resolution.

**Cross-domain marts**

* `customer_service_view` and `customer_360` in Gold with row-level policies.

---

## 26) Ready-to-Run Jobs (IDs to bind in Airflow)

* `JOB_BRONZE_AUTLOADER_ID` → parameterized system ingest (Jarvis, AMDM, Pega, CMDM).
* `JOB_SILVER_STANDARDIZE_ID` → domain-specific cleanse.
* `JOB_GOLD_MARTS_ID` → publish DIMs and FACTs (includes SCD2 logic).
* `JOB_RAG_BUILD_IDX_ID` → chunk, embed, and Vector Search upsert.
* `JOB_MAINTENANCE_ID` → scheduled OPTIMIZE, ZORDER, VACUUM rotation.

---

## 27) Next Steps (MT roll-out)

1. Apply network and cluster policies in dev; validate audit log delivery.
2. Stand up UC catalogs/schemas and groups; run grants automation.
3. Deploy Bronze ingest per domain; verify DQ gates and quarantine flow.
4. Implement CMDM SCD2 and key joins to Jarvis, AMDM, and Pega; align with Mandos mappings.
5. Enable RAG with domain filters and redaction; pilot via Teams bot and SFDC widget.
6. Turn on observability dashboards and cost tags; agree SLAs.
7. Promote to test and prod via Bundles with CAB approval.
