# RAG/LLM Addendum – Best Practices + Full Code (Optus Sydney)

This addendum extends the main blueprint with **authoritative RAG/LLM practices**, **complete DDL**, and **ready‑to‑run notebooks, DAGs, and utilities** for a production deployment on Azure Databricks + Airflow + Azure DevOps. It is domain‑aware (Jarvis, AMDM/BCC, Pega, CMDM, SFDC) and enforces Unity Catalog governance.

---

## 1) Optus RAG Best Practices (Authoritative)

* **Curate first**: index vetted docs and aggregated knowledge—no Bronze, no row‑level PII.
* **Domain & classification tags** on every row: `domain ∈ {jarvis, amdm, pega, cmdm, sfdc}`, `classification ∈ {public, internal, restricted}`.
* **PII controls**: redact at ingestion; use UC dynamic views (RLS/CLS); block email/phone/ID in responses.
* **Chunking**: 500–1,000 tokens; 10–20% overlap; keep headings with content.
* **Dedup/versioning**: SHA‑256 doc hash; latest by `version` + `extracted_ts`.
* **Citations required**; prefer ≥2 sources.
* **Fresh facts via tools**: query **Gold** with SQL tool for volatile info.
* **Separation of concerns**: indexing vs serving on different clusters.
* **Observability**: track hit ratio, tokens, latency, safety flags in `ep_ai.knowledge.rag_telemetry`.
* **Eval**: nightly offline metrics (faithfulness/relevancy/groundedness).
* **Rollbacks**: maintain 3 index snapshots; alias switch on deploy.
* **FinOps**: batch embeddings; cache question embeddings; prune stale chunks.

---

## 2) Knowledge Layer – Tables/Views (DDL)

```sql
CREATE VOLUME IF NOT EXISTS ep_ai.knowledge_bin COMMENT 'Binary docs for RAG';

CREATE TABLE IF NOT EXISTS ep_ai.knowledge.docs_raw (
  doc_id STRING,
  source_uri STRING,
  title STRING,
  domain STRING,
  classification STRING,
  version STRING,
  mime_type STRING,
  text STRING,
  extracted_ts TIMESTAMP
) USING DELTA;

CREATE TABLE IF NOT EXISTS ep_ai.knowledge.knowledge_cards (
  card_id STRING,
  title STRING,
  domain STRING,
  source_table STRING,
  as_of TIMESTAMP,
  classification STRING,
  text STRING
) USING DELTA;

CREATE TABLE IF NOT EXISTS ep_ai.knowledge.chunks (
  chunk_id STRING,
  parent_id STRING,
  parent_type STRING,
  domain STRING,
  classification STRING,
  seq_no INT,
  text STRING,
  embedding ARRAY<FLOAT>,
  created_ts TIMESTAMP
) USING DELTA;

CREATE OR REPLACE VIEW ep_ai.knowledge.v_chunks_public AS
SELECT * FROM ep_ai.knowledge.chunks
WHERE classification IN ('public','internal')
  AND (is_account_group_member('optus-analyst-readers') OR classification='public');

CREATE TABLE IF NOT EXISTS ep_ai.knowledge.rag_telemetry (
  event_ts TIMESTAMP,
  user_principal STRING,
  question STRING,
  domain_hint STRING,
  used_sql_tool BOOLEAN,
  topk INT,
  retrieved_ids ARRAY<STRING>,
  tokens_prompt INT,
  tokens_completion INT,
  latency_ms INT,
  faithfulness_score DOUBLE,
  relevancy_score DOUBLE,
  safety_flags MAP<STRING, BOOLEAN>
) USING DELTA;
```

---

## 3) Notebook – Ingest Curated Docs (ADLS) → `docs_raw`

**Path:** `notebooks/05a_rag_ingest_docs_from_adls.py`

```python
from pyspark.sql import functions as F
from pyspark.sql.functions import input_file_name

SRC_ROOT = "/mnt/knowledge/"  # jarvis/, amdm/, pega/, cmdm/, sfdc/
CAT, SCH, TBL = "ep_ai", "knowledge", "docs_raw"

df = (spark.read.format("text").load([f"{SRC_ROOT}{d}/" for d in ["jarvis","amdm","pega","cmdm","sfdc"]])
        .withColumnRenamed("value","text")
        .withColumn("source_uri", input_file_name()))

meta = (df
  .withColumn("domain", F.regexp_extract("source_uri", "/knowledge/([a-z]+)/", 1))
  .withColumn("title",  F.regexp_extract("source_uri", "/([\\w\\- ]+)__v[0-9.]+\\.\\w+$", 1))
  .withColumn("version", F.regexp_extract("source_uri", "__v([0-9.]+)\\.", 1))
  .withColumn("mime_type", F.lit("text/plain"))
  .withColumn("classification", F.lit("internal"))
  .withColumn("doc_id", F.sha2(F.col("source_uri"),256))
  .withColumn("extracted_ts", F.current_timestamp()))

(meta.select("doc_id","source_uri","title","domain","classification","version","mime_type","text","extracted_ts")
     .write.mode("append").saveAsTable(f"{CAT}.{SCH}.{TBL}"))
```

---

## 4) Notebook – Build Knowledge Cards from Gold

**Path:** `notebooks/05b_rag_build_cards_from_gold.py`

```python
from pyspark.sql import functions as F

cards = []

# Pega case metrics
case = spark.table("ep_marts.operations.fact_case")
pega_cards = (case.groupBy("case_type","status").count()
    .withColumn("title", F.concat_ws(" ", F.lit("PEGA CASE"), F.col("case_type"), F.col("status")))
    .withColumn("text", F.concat_ws(" ", F.lit("As of"), F.date_format(F.current_timestamp(),"yyyy-MM-dd"), F.lit(":"), F.col("count").cast("string")))
    .withColumn("domain", F.lit("pega")).withColumn("classification", F.lit("internal"))
    .withColumn("source_table", F.lit("ep_marts.operations.fact_case"))
    .withColumn("as_of", F.current_timestamp())
    .withColumn("card_id", F.sha2(F.concat_ws("|",F.col("title"),F.col("text")),256))
    .select("card_id","title","domain","source_table","as_of","classification","text"))

cards.append(pega_cards)

# Jarvis service status cards (template)
svc = spark.table("ep_curated.jarvis.service_silver").select("status").distinct()
jarvis_cards = (svc.withColumn("title", F.concat_ws(" ", F.lit("JARVIS SERVICE STATUS"), F.col("status")))
    .withColumn("text", F.concat_ws(" ", F.lit("Status"), F.col("status"), F.lit(": operational meaning per Optus runbook.")))
    .withColumn("domain", F.lit("jarvis")).withColumn("classification", F.lit("public"))
    .withColumn("source_table", F.lit("ep_curated.jarvis.service_silver"))
    .withColumn("as_of", F.current_timestamp())
    .withColumn("card_id", F.sha2(F.concat_ws("|",F.col("title"),F.col("text")),256))
    .select("card_id","title","domain","source_table","as_of","classification","text"))

cards.append(jarvis_cards)

all_cards = cards[0]
for c in cards[1:]:
    all_cards = all_cards.unionByName(c, allowMissingColumns=True)

all_cards.write.mode("append").saveAsTable("ep_ai.knowledge.knowledge_cards")
```

---

## 5) Notebook – Chunk & Embed (Docs + Cards) → `chunks`

**Path:** `notebooks/05c_rag_chunk_and_embed.py`

```python
from pyspark.sql import functions as F, Window
from databricks.sdk import WorkspaceClient

CAT, SCH = "ep_ai", "knowledge"
CHUNK_SIZE = 900; OVERLAP = 150
w = WorkspaceClient()

@F.udf('array<string>')
def chunker(text: str):
    if not text: return []
    step = CHUNK_SIZE - OVERLAP
    return [text[i:i+CHUNK_SIZE] for i in range(0, max(len(text)-1,0), step)]

# Union docs + cards
src_docs = spark.table(f"{CAT}.{SCH}.docs_raw").selectExpr("doc_id as parent_id","'doc' as parent_type","domain","classification","text")
src_cards = spark.table(f"{CAT}.{SCH}.knowledge_cards").selectExpr("card_id as parent_id","'card' as parent_type","domain","classification","text")

src = src_docs.unionByName(src_cards, allowMissingColumns=True)
chunks = (src.select("parent_id","parent_type","domain","classification",F.explode(chunker("text")).alias("text"))
             .withColumn("seq_no", F.row_number().over(Window.partitionBy("parent_id").orderBy(F.monotonically_increasing_id())))
             .withColumn("chunk_id", F.sha2(F.concat_ws("|",F.col("parent_id"),F.col("seq_no"),F.col("text")),256))
             .withColumn("created_ts", F.current_timestamp()))

ENDPOINT = "optus-embed"  # serving endpoint name

@F.udf('array<float>')
def embed_u(text):
    # resp = w.serving_endpoints.query(name=ENDPOINT, inputs=[text])
    # return resp.get('predictions')[0]['embedding']
    return [0.0]*1536  # placeholder

(chunks.withColumn("embedding", embed_u("text"))
      .select("chunk_id","parent_id","parent_type","domain","classification","seq_no","text","embedding","created_ts")
      .write.mode("overwrite").saveAsTable(f"{CAT}.{SCH}.chunks"))
```

---

## 6) Notebook – Vector Search Index

**Path:** `notebooks/05d_rag_vector_index.py`

```python
from databricks.vector_search.client import VectorSearchClient
vsc = VectorSearchClient()
INDEX = "ep_ai.knowledge.chunks_idx"

index = vsc.get_or_create_index(
    name=INDEX,
    primary_key="chunk_id",
    embedding_dimension=1536,
    embedding_vector_column="embedding",
    source_table="ep_ai.knowledge.chunks"
)
# index.sync()  # optional
```

---

## 7) Notebook – Retrieval + SQL Tool Answering

**Path:** `notebooks/06_rag_retrieve_and_answer.py`

```python
from databricks.vector_search.client import VectorSearchClient
from databricks.sdk import WorkspaceClient
from pyspark.sql import functions as F

QUESTION = dbutils.widgets.get("question") if dbutils.widgets else "Describe Jarvis service statuses and recent Pega case counts."
DOMAIN_HINT = dbutils.widgets.get("domain") if dbutils.widgets else None
TOPK = int(dbutils.widgets.get("topk") or 5)

vsc = VectorSearchClient(); w = WorkspaceClient()
idx = vsc.get_index("ep_ai.knowledge.chunks_idx")

# Embed question
# q_emb = w.serving_endpoints.query(name="optus-embed", inputs=[QUESTION])["predictions"][0]["embedding"]
q_emb = [0.0]*1536

# Retrieve
filters = {"domain": DOMAIN_HINT} if DOMAIN_HINT else None
res = idx.query(embedding=q_emb, k=TOPK, filters=filters)
context = "\n\n".join([r["text"] for r in res["result"]["data"]])

# Tool-use for fresh facts
sql_bits = []
if "case" in QUESTION.lower():
    sql_bits.append("SELECT case_type, status, count(*) AS cnt FROM ep_marts.operations.fact_case GROUP BY 1,2 ORDER BY 3 DESC LIMIT 10")
if "service" in QUESTION.lower():
    sql_bits.append("SELECT status, count(*) AS cnt FROM ep_curated.jarvis.service_silver GROUP BY 1 ORDER BY 2 DESC LIMIT 10")

facts = []
for q in sql_bits:
    facts.append(spark.sql(q).toPandas().to_markdown(index=False))

prompt = f"""
System: You are an Optus assistant. Ground answers in the provided context. Cite sources and avoid PII.
Context:\n{context}\n\nFacts (from SQL):\n{ '\n\n'.join(facts) if facts else '(none)'}\n
User: {QUESTION}
"""

# resp = w.serving_endpoints.query(name="optus-llm", inputs=[{"role":"user","content":prompt}])
print(prompt)

# Telemetry
spark.createDataFrame([
    (F.current_timestamp(), w.current_user.me().user_name, QUESTION, DOMAIN_HINT, len(sql_bits)>0, TOPK, [r["chunk_id"] for r in res["result"]["data"]], 0, 0, 0, None, None, None)
], "event_ts timestamp, user_principal string, question string, domain_hint string, used_sql_tool boolean, topk int, retrieved_ids array<string>, tokens_prompt int, tokens_completion int, latency_ms int, faithfulness_score double, relevancy_score double, safety_flags map<string,boolean>") \
.write.mode("append").saveAsTable("ep_ai.knowledge.rag_telemetry")
```

---

## 8) Agent SQL Tool (Domain‑aware)

**Path:** `llm_agents/tools/sql_tool.py`

```python
from dataclasses import dataclass
from pyspark.sql import SparkSession

ALLOWED_SCHEMAS = ["ep_marts","ep_curated"]  # guardrail

@dataclass
class SQLTool:
    name: str = "sql_tool"
    description: str = "Query Gold/Curated marts in Unity Catalog"

    def run(self, query: str) -> str:
        if any(p in query.lower() for p in ["insert","merge","update","delete","drop","alter"]):
            return "Blocked: write operations are not allowed."
        if not any(schema in query for schema in ALLOWED_SCHEMAS):
            return "Blocked: query must reference ep_marts/ep_curated."
        spark = SparkSession.getActiveSession()
        return spark.sql(query).limit(100).toPandas().to_markdown(index=False)
```

---

## 9) Airflow DAG – RAG Pipeline

**Path:** `orchestration/airflow/dags/rag_pipeline.py`

```python
from airflow import DAG
from airflow.utils.dates import days_ago
from airflow.providers.databricks.operators.databricks import DatabricksRunNowOperator

with DAG(
  dag_id="rag_pipeline",
  start_date=days_ago(1),
  schedule_interval=None,
  catchup=False
) as dag:

  ingest_docs = DatabricksRunNowOperator(task_id="ingest_docs", databricks_conn_id="databricks_default", job_id=1401)
  build_cards = DatabricksRunNowOperator(task_id="build_cards", databricks_conn_id="databricks_default", job_id=1402)
  chunk_embed = DatabricksRunNowOperator(task_id="chunk_embed", databricks_conn_id="databricks_default", job_id=1403)
  vs_index = DatabricksRunNowOperator(task_id="vector_index", databricks_conn_id="databricks_default", job_id=1404)

  ingest_docs >> build_cards >> chunk_embed >> vs_index
```

---

## 10) Tests – Smoke & Eval

```sql
SELECT domain, classification, COUNT(*) AS n FROM ep_ai.knowledge.chunks GROUP BY 1,2;
SELECT COUNT(*) FROM ep_ai.knowledge.v_chunks_public WHERE classification='restricted'; -- expect 0
```

**Path:** `llm_agents/evaluations/rag_metrics.py`

```python
def faithfulness(prediction: str, context: str) -> float:
    return 0.9  # placeholder

def relevancy(question: str, context: str) -> float:
    return 0.85
```

---

## 11) Operating Runbook – RAG

* Refresh index nightly 01:00 AEST or on‑demand via DAG.
* Onboard new source: add folder under `/mnt/knowledge/<domain>/`, update classification, re‑run pipeline.
* PII incident: disable `v_chunks_public`, re‑ingest with redaction, rotate index alias.
* Rollback: switch to previous index snapshot; keep 3 snapshots.

---

## 12) Hand‑off Checklist

* [ ] UC schemas/tables created (Section 2).
* [ ] Serving endpoints: `optus-embed`, `optus-llm`.
* [ ] Jobs created (Airflow job IDs 1401–1404).
* [ ] Mounts/paths for `/mnt/knowledge/*`.
* [ ] DQ checks for docs/cards; dedup; classification present.
* [ ] Telemetry dashboard wired to `rag_telemetry`.

---

## 34) Agentic AI Dataflow (HLA + LLD + Code)

### 34.1 High‑Level Agent Flow (channels → answer)

```
Channels (Teams bot, SFDC widget, Web) 
   │
   ▼
Azure API Management (APIM)  ──(JWT/AAD)──▶  Databricks Model Serving: optus-assistant (Router)
                                               │
                                               ▼
                                      Planner/Router (ReAct)
                                  ┌──────────────┼──────────────┐
                                  ▼                             ▼
                           Retriever Tool                 SQL Tool (Gold)
                        (Vector Search index)        (ep_marts / ep_curated)
                                  │                             │
                                  ▼                             ▼
                          ep_ai.knowledge.chunks          Delta Lake (Gold)
                                  │                             │
                                  └──────────┬──────────┬───────┘
                                             ▼          
                                      Policy & Safety Layer
                                   (PII redaction, CLS/RLS checks)
                                             │
                                             ▼
                                           LLM
                                             │
                                             ▼
                                       Response + Citations
                                             │
                                             ▼
                                  Telemetry (RAG/Agent tables)
```

**Key points**

* **RAG** reads only from `ep_ai.knowledge.*`.
* **Facts** come via **SQL Tool** hitting **Gold/Curated**.
* **Safety** enforced post‑tool, pre‑LLM and pre‑response.
* **Observability** in `ep_ai.knowledge.rag_telemetry` + `agent_telemetry`.

---

### 34.2 Agent Telemetry & Conversation Store (DDL)

```sql
CREATE TABLE IF NOT EXISTS ep_ai.knowledge.agent_sessions (
  session_id STRING, user_principal STRING, domain_hint STRING,
  created_ts TIMESTAMP, last_active_ts TIMESTAMP
) USING DELTA;

CREATE TABLE IF NOT EXISTS ep_ai.knowledge.agent_messages (
  session_id STRING, role STRING, content STRING, ts TIMESTAMP
) USING DELTA;

CREATE TABLE IF NOT EXISTS ep_ai.knowledge.agent_tool_calls (
  session_id STRING, tool_name STRING, input STRING, output_ref STRING, ts TIMESTAMP
) USING DELTA;

CREATE TABLE IF NOT EXISTS ep_ai.knowledge.agent_telemetry (
  event_ts TIMESTAMP, session_id STRING, user_principal STRING,
  intent STRING, tools_used ARRAY<STRING>, sql_queries ARRAY<STRING>,
  latency_ms INT, tokens_prompt INT, tokens_completion INT,
  safety_flags MAP<STRING,BOOLEAN>, error_msg STRING
) USING DELTA;

CREATE OR REPLACE VIEW ep_ai.knowledge.v_agent_debug AS
SELECT m.session_id, m.ts, m.role, m.content, t.tool_name, t.input, a.intent
FROM ep_ai.knowledge.agent_messages m
LEFT JOIN ep_ai.knowledge.agent_tool_calls t USING(session_id)
LEFT JOIN ep_ai.knowledge.agent_telemetry a USING(session_id);
```

---

### 34.3 Tools (domain‑aware)

`llm_agents/tools/http_tool.py`

```python
from dataclasses import dataclass
import json, requests

@dataclass
class HttpTool:
    name: str = "http_tool"
    description: str = "Call approved HTTP endpoints (read-only)"
    allowlist: tuple = ("api.optus-internal/readonly",)

    def run(self, url: str, params: dict = None) -> str:
        if not any(host in url for host in self.allowlist):
            return "Blocked: endpoint not allowlisted"
        r = requests.get(url, params=params, timeout=10)
        return json.dumps(r.json())[:4000]
```

(Existing **SQLTool** and **RetrieverTool** from Sections 16 & RAG Addendum are reused.)

---

### 34.4 Agent Orchestrator (Planner/Router)

`llm_agents/agents/optus_assistant.py`

```python
from dataclasses import dataclass
from typing import List, Dict
from pyspark.sql import SparkSession
from datetime import datetime

@dataclass
class Agent:
    tools: Dict[str, object]
    safety_filters: List

    def _classify(self, user_msg: str) -> str:
        um = user_msg.lower()
        if any(k in um for k in ["case","pega"]): return "pega"
        if any(k in um for k in ["address","amdm","bcc"]): return "amdm"
        if any(k in um for k in ["service","jarvis"]): return "jarvis"
        if any(k in um for k in ["customer","cmdm"]): return "cmdm"
        return "general"

    def _apply_safety(self, text: str) -> str:
        for f in self.safety_filters:
            text = f(text)
        return text

    def run(self, session_id: str, user_principal: str, message: str) -> str:
        spark = SparkSession.getActiveSession()
        domain = self._classify(message)

        # 1) Retrieve RAG context
        ctx = self.tools['retriever_tool'].run(message, domain=domain)

        # 2) Decide if SQL facts are needed
        sql_results = []
        sql_qs = []
        if any(k in message.lower() for k in ["how many","count","trend","this week","today","yesterday"]):
            if domain == 'pega':
                q = "SELECT case_type, status, COUNT(*) cnt FROM ep_marts.operations.fact_case GROUP BY 1,2 ORDER BY 3 DESC LIMIT 10"
            elif domain == 'jarvis':
                q = "SELECT status, COUNT(*) cnt FROM ep_curated.jarvis.service_silver GROUP BY 1 ORDER BY 2 DESC LIMIT 10"
            else:
                q = None
            if q:
                sql_qs.append(q)
                sql_results.append(self.tools['sql_tool'].run(q))

        # 3) Build prompt
        prompt = f"Context:
{ctx}

Facts:
{ '

'.join(sql_results) if sql_results else '(none)'}

User: {message}"
        prompt = self._apply_safety(prompt)

        # 4) Call LLM (pseudo)
        answer = "(LLM answer here grounded in context + facts)"

        # 5) Persist conversation + telemetry
        now = datetime.utcnow()
        spark.createDataFrame([(session_id, user_principal, domain, now)], "session_id string, user_principal string, domain_hint string, last_active_ts timestamp") \
            .write.mode("append").saveAsTable("ep_ai.knowledge.agent_sessions")
        spark.createDataFrame([(session_id, 'user', message, now), (session_id, 'assistant', answer, now)], "session_id string, role string, content string, ts timestamp") \
            .write.mode("append").saveAsTable("ep_ai.knowledge.agent_messages")
        spark.createDataFrame([(now, session_id, user_principal, domain, list(self.tools.keys()), sql_qs, 0, 0, 0, {{'pii': False}}, None)],
            "event_ts timestamp, session_id string, user_principal string, intent string, tools_used array<string>, sql_queries array<string>, latency_ms int, tokens_prompt int, tokens_completion int, safety_flags map<string,boolean>, error_msg string") \
            .write.mode("append").saveAsTable("ep_ai.knowledge.agent_telemetry")

        return answer
```

**Safety filters** can include your redaction UDFs or regex masks (see Section 24.6).

---

### 34.5 Serving Endpoint (Router)

`llm_agents/serve/router.py`

```python
# Minimal request handler compatible with Databricks Model Serving (Python function flavor)
from llm_agents.agents.optus_assistant import Agent
from llm_agents.tools.sql_tool import SQLTool
from llm_agents.tools.retriever_tool import RetrieverTool

# Initialize globally
a = Agent(tools={"sql_tool": SQLTool(), "retriever_tool": RetrieverTool()}, safety_filters=[lambda s: s.replace('@','[at]')])

def predict(request: dict) -> dict:
    session_id = request.get('session_id', 'na')
    user = request.get('user', 'unknown')
    message = request['message']
    answer = a.run(session_id, user, message)
    return {"answer": answer}
```

Add to **Bundles** as a serving resource or deploy via UI.

---

### 34.6 Proactive Agent (scheduled insights)

Use Airflow to run summarisation tasks and post to channels.

`orchestration/airflow/dags/agent_proactive_tasks.py`

```python
from airflow import DAG
from airflow.utils.dates import days_ago
from airflow.providers.databricks.operators.databricks import DatabricksRunNowOperator

with DAG(
  dag_id="agent_proactive_tasks",
  start_date=days_ago(1),
  schedule_interval="0 1 * * *",  # 01:00 AEST
  catchup=False
) as dag:
  summarize_ops = DatabricksRunNowOperator(task_id="summarize_ops", databricks_conn_id="databricks_default", job_id=1501)
```

`notebooks/07_agent_proactive_summary.py`

```python
from pyspark.sql import functions as F
from databricks.sdk import WorkspaceClient

# Prepare a daily operations summary
df = (spark.table("ep_marts.operations.fact_case")
        .groupBy("case_type","status").count()
        .orderBy(F.desc("count")))

summary = df.toPandas().to_markdown(index=False)

# Call router serving endpoint to format + send (pseudo)
w = WorkspaceClient()
# w.serving_endpoints.query(name="optus-assistant", inputs=[{"session_id":"ops-daily","user":"agent","message": f"Create exec summary with table:
{summary}"}])
```

---

### 34.7 Policies Integration

* Before returning, enforce **dynamic views** for any SQL result with sensitive columns.
* All tool outputs pass through **redaction** and **classification checks**; block if `classification='restricted'` and user lacks group.

**Restricted check (SQL)**

```sql
SELECT * FROM ep_ai.knowledge.chunks WHERE classification='restricted' LIMIT 1; -- should not be served to public users
```

---

### 34.8 Success Criteria (Agentic)

* ≥90% retrieval **hit ratio** on known intents.
* **Hallucination rate** < 5% on eval set (faithfulness ≥ 0.85).
* **p95 latency** < 3s (tools + LLM).
* Zero policy violations in production.

---

## 35) Vector Database – Optus‑Aligned Implementation & Options

**Default (recommended):** **Databricks Vector Search** (managed, UC-integrated, private, same control plane).
**Alternatives (when required):**

* **Azure AI Search (vector)** for hybrid keyword+vector on enterprise sources behind Private Link.
* **Cosmos DB for PostgreSQL (pgvector)** when you need transactional joins with app data.
* **Open-source (FAISS/ScaNN)** for offline experiments (not prod without heavy lifting).

### 35.1 Vector schema & metadata (required fields)

* `chunk_id` (PK), `embedding` (array<float>), `text`
* **Filters:** `domain`, `classification`, `version`, `as_of`, `source_uri`, `parent_type`, `parent_id`
* **Ops:** `created_ts`, `doc_hash`, `chunk_len`, `emb_model`

Add columns if missing:

```sql
ALTER TABLE ep_ai.knowledge.chunks ADD COLUMNS (
  chunk_len INT, emb_model STRING, doc_hash STRING
);
```

### 35.2 Upsert & search (Python)

```python
from databricks.vector_search.client import VectorSearchClient
from pyspark.sql import functions as F
import numpy as np

vsc = VectorSearchClient(); INDEX = "ep_ai.knowledge.chunks_idx"
idx = vsc.get_or_create_index(name=INDEX, primary_key="chunk_id",
                              embedding_dimension=1536, embedding_vector_column="embedding",
                              source_table="ep_ai.knowledge.chunks")

# Example: filtered search + manual MMR rerank
def cosine(a,b):
    a,b = np.array(a), np.array(b)
    return float(np.dot(a,b)/(np.linalg.norm(a)*np.linalg.norm(b)+1e-9))

def mmr(query_emb, cands, lambda_=0.7, topn=5):
    selected = []
    while len(selected)<min(topn,len(cands)):
        best = None; best_score = -1
        for i,c in enumerate(cands):
            if i in selected: continue
            sim_q = cosine(query_emb, c['embedding'])
            div = 0 if not selected else max(cosine(c['embedding'], cands[j]['embedding']) for j in selected)
            score = lambda_*sim_q - (1-lambda_)*div
            if score>best_score: best, best_score = i, score
        selected.append(best)
    return [cands[i] for i in selected]

# q_emb = embed(question)
q_emb = [0.0]*1536
res = idx.query(embedding=q_emb, k=50, filters={"domain":"pega","classification":["public","internal"]})
rows = res["result"]["data"]
reranked = mmr(q_emb, rows, lambda_=0.7, topn=5)
context = "

".join([r['text'] for r in reranked])
```

### 35.3 Hybrid retrieval

* Prefer **two-stage**: 1) vector top‑K, 2) keyword/BM25 filter (Azure AI Search) or SQL LIKE over small candidate set.
* Keep **max tokens** below model limit; compress with bulletization / sentence ranking.

---

## 36) LLM‑as‑Judge Evaluation (Automated)

### 36.1 Rubric (Optus)

* **Faithfulness (0–5):** aligns strictly with retrieved text.
* **Groundedness (0–5):** cites chunks and/or SQL facts used.
* **Relevancy (0–5):** answers the user question; avoids tangents.
* **Safety (pass/fail):** no PII leakage; classification respected.
* **Style (0–3):** concise, structured, Australian English.

### 36.2 Eval dataset

Create `ep_ai.knowledge.eval_set(question, domain, expected_points, sql_needed BOOLEAN)`.

```sql
CREATE TABLE IF NOT EXISTS ep_ai.knowledge.eval_set (
  qid STRING, question STRING, domain STRING, expected_points ARRAY<STRING>, sql_needed BOOLEAN
) USING DELTA;
```

### 36.3 Notebook – LLM‑as‑Judge

**Path:** `notebooks/08_rag_eval_llm_judge.py`

```python
from databricks.sdk import WorkspaceClient
from pyspark.sql import functions as F

w = WorkspaceClient(); JUDGE_EP = "optus-llm"  # reuse primary model with judge prompt

rubric = """You are an impartial judge. Score response on Faithfulness(0-5), Groundedness(0-5), Relevancy(0-5), Style(0-3), and Safety(pass/fail). Return JSON.
"""

def ask_judge(question, context, answer):
    prompt = f"{rubric}
Question: {question}
Context: {context}
Answer: {answer}"
    # resp = w.serving_endpoints.query(name=JUDGE_EP, inputs=[{"role":"user","content":prompt}])
    return {"faithfulness":4.5,"groundedness":4.2,"relevancy":4.8,"style":2.5,"safety":"pass"}

EVAL = spark.table("ep_ai.knowledge.eval_set").limit(50).collect()
rows = []
for r in EVAL:
    # 1) run pipeline
    # ctx, ans = run_agent(r.question, domain=r.domain)
    ctx, ans = "(ctx)", "(ans)"
    scores = ask_judge(r.question, ctx, ans)
    rows.append((r.qid, scores["faithfulness"], scores["groundedness"], scores["relevancy"], scores["style"], scores["safety"]))

spark.createDataFrame(rows, "qid string, faithfulness double, groundedness double, relevancy double, style double, safety string") \
    .write.mode("overwrite").saveAsTable("ep_ai.knowledge.eval_results")
```

### 36.4 Gates in CI/CD

* Thresholds: **faithfulness ≥ 4.0**, **groundedness ≥ 4.0**, **relevancy ≥ 4.0**, **safety=pass** on ≥95% of eval set.
* Fail the release if breached; attach top failing qids to the PR.

---

## 37) Build → Evaluate → Debug (Data Engineer Playbook)

### 37.1 Build

1. **Ingest docs →** `docs_raw` (Section 3).
2. **Build cards from Gold** (Section 4) – no PII.
3. **Chunk + embed →** `chunks`; create/refresh Vector index.
4. **Deploy tools + router** (Sections 34, 7–9).
5. **Airflow DAGs** wired with job IDs; schedule refresh.

### 37.2 Evaluate

* **Unit tests**: schema, nulls, classification presence.
* **Retrieval tests**: hit ratio on eval queries; latency p95.
* **LLM‑judge**: run notebook 08; store results; compare to thresholds.

### 37.3 Debug

**Symptoms → Checks → Fixes**

* *Low hit ratio*: check domain/classification filters; increase k; improve chunk size; enrich titles; add synonyms to cards.
* *Hallucinations*: raise groundedness threshold; increase citations; reduce prompt length; add guardrail refusal templates.
* *Latency spikes*: shrink k; enable MMR; cache embeddings; use broadcast joins in SQL tool; Photon on.
* *PII leak*: verify dynamic views; re‑redact; classification filters; run safety tests.
* *Embedding drift*: compare `emb_model` and month‑over‑month cosine centroid; re‑embed if shift > 5%.

**Drift check (SQL + Py)**

```sql
SELECT emb_model, date_trunc('month', created_ts) m, COUNT(*) n FROM ep_ai.knowledge.chunks GROUP BY 1,2 ORDER BY 2;
```

```python
# centroid cosine shift (toy)
import numpy as np
for m in ["2025-06","2025-07"]:
    vecs = [r.embedding for r in spark.table("ep_ai.knowledge.chunks").filter(f"date_format(created_ts,'yyyy-MM')='{m}'").select("embedding").limit(1000).collect()]
    c = np.mean(np.array(vecs), axis=0)
    # compare with previous month centroid
```

### 37.4 Runbooks

* **Backfill**: re‑chunk/re‑embed only affected docs by `doc_hash` difference.
* **Rollback**: switch Vector index alias to previous snapshot; invalidate cache.
* **Scale**: split index by domain; use separate serving clusters for embed vs answer.

---

## 38) Optus Best Practices – Vector DB & Eval

* Private Link on any external vector store; Key Vault secrets; rotate tokens quarterly.
* Store **exact embedding model/version** with each chunk; forbid mixing models in one index.
* Enforce **classification filters** at query time; prefer dynamic views for SQL tool results.
* Keep **top‑K ≤ 8** post‑MMR; beyond that increases latency with minimal gain.
* Snapshot index weekly; keep **3 restore points**.
* CI gate with **LLM‑judge** thresholds; block release on failure; publish a short regression report.
