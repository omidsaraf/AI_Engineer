# NILOOMID — AI\_Engineer (Refined, Grade‑A, Step‑by‑Step)

> **Goal:** A single, production‑ready blueprint that merges and validates all content in your `AI_Engineer` repo into a coherent, end‑to‑end project with HLA, LLD, Data Flows, code templates, governance, CI/CD, tests, and runbooks. Optimized for **Azure Databricks + Delta/Unity Catalog**, **Airflow** orchestration, **Azure DevOps/GitHub Actions** CI/CD, and **Agentic/RAG** workloads.

---

## 0) Executive Summary & SLOs

**Scope**

* Data lakehouse (Bronze → Silver → Gold) on Databricks/Delta (Unity Catalog enabled)
* Batch + Streaming ingestion, validation (Great Expectations), lineage, logging
* RAG search over curated text (Gold/Text) with vector DB (FAISS/Qdrant)
* Agentic workflows (LangGraph) + FastAPI service layer + Observability
* Infra as Code (Terraform/Bicep), CI/CD (GitHub Actions/Azure Pipelines)

**SLOs & Guardrails**

* p95 API latency ≤ 2.5s; embedding throughput ≥ 1k chunks/min (autoscale)
* DQ pass rate ≥ 99% at Silver gates; schema drift blocked by CI
* RAG: retrieval hit‑rate ≥ 0.85; faithfulness ≥ 0.75; hallucination ≤ 5%
* Cost budget: ≤ \$X/1k requests; storage lifecycle policies active

---

## Step‑by‑Step Build (Order of Operations + Validation Gates)

> Everything in strict order with **Inputs → Actions (commands/code) → Outputs → Validation Gate**. Copy/paste runnable snippets are included. Replace `<PLACEHOLDERS>`.

### Phase −2 — Local Tooling & Repo Bootstrap

**Inputs:** GitHub repo, workstation.
**Actions:**

```bash
# tools
brew install azure-cli terraform databricks jq python@3.11 node
pip install databricks-cli databricks-sdk great_expectations pytest

# repo
git clone <your_repo_url> && cd ai-pipeline
python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt
```

**Outputs:** Toolchain ready.
**Validation:** `az version`, `databricks --version`, `terraform -v` OK.

### Phase −1 — Scope, SLOs, Policies

**Inputs:** Business goals, risk posture.
**Actions:** Finalize SLOs (p95 ≤ 2.5s; GE ≥ 99%; hit‑rate ≥ 0.85; cost ≤ budget); draft ADRs and RBAC matrix.
**Outputs:** `/docs/adr/*.md`, `/docs/policies/*.md`.
**Validation:** Approved in PR with sign‑off.

### Phase 0 — Naming, Variables, Tags

**Inputs:** Org conventions.
**Actions:** Choose names once: `niloomid-{dev|test|prod}`, `rg-niloomid-{env}`; tag `CostCenter=DE`, `Owner=DataPlatform`.
**Outputs:** `/infra/vars/{env}.tfvars`.
**Validation:** Names applied consistently in TF plan.

### Phase 1 — Infra: Azure RG, VNet, Storage, KV, PE (Terraform)

**Inputs:** Subscription ID, region.
**Actions:** Create minimal infra.

```hcl
# infra/terraform/main.tf
terraform { required_providers { azurerm = { source = "hashicorp/azurerm", version = "~> 3.114" } } }
provider "azurerm" { features {} }

variable "env" {}
locals { name = "niloomid-${var.env}" }

resource "azurerm_resource_group" "rg" { name = "rg-${local.name}" location = "australiaeast" }
resource "azurerm_virtual_network" "vnet" { name = "vnet-${local.name}" address_space = ["10.10.0.0/16"] resource_group_name = azurerm_resource_group.rg.name location = azurerm_resource_group.rg.location }
resource "azurerm_subnet" "private" { name = "snet-private" resource_group_name = azurerm_resource_group.rg.name virtual_network_name = azurerm_virtual_network.vnet.name address_prefixes = ["10.10.1.0/24"] private_endpoint_network_policies_enabled = true }
resource "azurerm_storage_account" "sa" { name = "st${var.env}niloomid${random_string.suff.result}" resource_group_name = azurerm_resource_group.rg.name location = azurerm_resource_group.rg.location account_tier = "Standard" account_replication_type = "LRS" account_kind = "StorageV2" is_hns_enabled = true }
resource "random_string" "suff" { length = 5 lower = true special = false }
resource "azurerm_key_vault" "kv" { name = "kv-${local.name}" resource_group_name = azurerm_resource_group.rg.name location = azurerm_resource_group.rg.location tenant_id = data.azurerm_client_config.current.tenant_id sku_name = "standard" purge_protection_enabled = true }
data "azurerm_client_config" "current" {}

# Databricks workspace + Access Connector
resource "azurerm_databricks_workspace" "dbx" { name = "dbx-${local.name}" resource_group_name = azurerm_resource_group.rg.name location = azurerm_resource_group.rg.location pricing_tier = "premium" }
resource "azurerm_databricks_access_connector" "ac" { name = "ac-${local.name}" resource_group_name = azurerm_resource_group.rg.name location = azurerm_resource_group.rg.location identity { type = "SystemAssigned" } }

# RBAC: Access Connector to Storage (for UC external locations)
resource "azurerm_role_assignment" "ac_sa_contrib" {
  scope                = azurerm_storage_account.sa.id
  role_definition_name = "Storage Blob Data Contributor"
  principal_id         = azurerm_databricks_access_connector.ac.identity[0].principal_id
}
```

**Outputs:** RG, VNet/Subnet, ADLS Gen2, Key Vault, Workspace, Access Connector, RBAC.
**Validation:** `terraform apply` succeeds; RBAC assignment propagates.

### Phase 2 — Private DNS & Private Endpoints (optional but recommended)

**Inputs:** VNet/subnet from Phase 1.
**Actions:** Create Private Endpoints for Storage & Key Vault; link Private DNS zones. (Add TF resources `azurerm_private_endpoint` + `azurerm_private_dns_zone`.)
**Outputs:** Private access only.
**Validation:** `nslookup <vault>.privatelink.vaultcore.azure.net` resolves; public egress blocked by NSG.

### Phase 3 — Unity Catalog Bootstrap (SQL)

**Inputs:** Access Connector principal id, Storage account URL.
**Actions:** In Databricks SQL Warehouse, run:

```sql
CREATE STORAGE CREDENTIAL sc_niloomid
  WITH AZURE_MANAGED_IDENTITY
  COMMENT 'Access Connector for UC';

CREATE EXTERNAL LOCATION loc_landing
  URL 'abfss://landing@<STORAGE>.dfs.core.windows.net/'
  WITH STORAGE CREDENTIAL sc_niloomid
  COMMENT 'Landing zone';

CREATE CATALOG IF NOT EXISTS niloomid_dev;
USE CATALOG niloomid_dev;
CREATE SCHEMA IF NOT EXISTS raw;
CREATE SCHEMA IF NOT EXISTS clean;
CREATE SCHEMA IF NOT EXISTS gold;
CREATE SCHEMA IF NOT EXISTS meta;
CREATE SCHEMA IF NOT EXISTS ops;

GRANT USE CATALOG ON CATALOG niloomid_dev TO `de_admin`, `de_pipeline`, `data_analyst`;
GRANT SELECT ON SCHEMA clean TO `data_analyst`;
```

**Outputs:** UC credential, external location, catalog/schemas, grants.
**Validation:** `SHOW EXTERNAL LOCATIONS; SHOW GRANTS ON CATALOG niloomid_dev;` show entries.

### Phase 4 — Secrets & Scopes (Key Vault–backed)

**Inputs:** Key Vault resource ID and DNS name.
**Actions:**

```bash
# OIDC login (prefer over PATs)
az login
# Create KV-backed secret scope
DATABRICKS_HOST="https://<workspace-url>" databricks secrets create-scope \
  --scope kv-niloomid --scope-backend-type AZURE_KEYVAULT \
  --resource-id "/subscriptions/<sub>/resourceGroups/rg-niloomid-dev/providers/Microsoft.KeyVault/vaults/kv-niloomid-dev" \
  --dns-name "https://kv-niloomid-dev.vault.azure.net/"
```

**Outputs:** Secret scope `kv-niloomid`.
**Validation:** In notebook: `dbutils.secrets.get(scope="kv-niloomid", key="<key>")` works.

### Phase 5 — Cluster Policies & Workflows

**Inputs:** Policy JSON (*see §15*).
**Actions:** Create a **Single User + Photon** policy; enforce autotermination, tags. Register Workflows skeleton (*§30*).
**Outputs:** Policy ID; job/workflow IDs.
**Validation:** Job clusters inherit policy; disallowed edits blocked.

### Phase 6 — Data Contracts & DQ Suites

**Inputs:** Source schema & SLAs.
**Actions:** Add `/contracts/events.yml` (*§16*). Initialize **Great Expectations** and author suites (*§17.1*).
**Outputs:** Contracts + suites.
**Validation:** Local `pytest` + GE run; CI contract diff gate enabled.

### Phase 7 — Bronze Ingestion (Streaming & Batch)

**Inputs:** Landing path, schema location.
**Actions:** Run *Bronze notebook* (*§3.1*). Ensure checkpointing.
**Outputs:** `raw.events_bronze` table.
**Validation:** Stream active; lag < 5m; schema in `_schemas/events`.

### Phase 8 — Silver Cleanse + GE Gate

**Inputs:** Bronze, GE suites.
**Actions:** Run *Silver notebook* (*§3.2*); route failures to `ops.quarantine_events`.
**Outputs:** `clean.events_silver`, quarantine table.
**Validation:** GE pass ≥ 99%; constraints enforced (*§29*).

### Phase 9 — Gold KPIs & Curated Text

**Inputs:** Silver.
**Actions:** Run *Gold SQL* (*§3.3*).
**Outputs:** `gold.kpi_daily`, `gold.docs_text`.
**Validation:** Row counts & partitions as expected.

### Phase 10 — Vector DB & Index

**Inputs:** `gold.docs_text`.
**Actions:**

* **FAISS** (in‑process) with *`src/embed.py`*; or
* **Qdrant** via Docker:

```yaml
# docker/qdrant/docker-compose.yml
version: '3.8'
services:
  qdrant:
    image: qdrant/qdrant:latest
    ports: ["6333:6333", "6334:6334"]
    volumes: ["./qdrant_storage:/qdrant/storage"]
```

**Outputs:** FAISS index or Qdrant collection.
**Validation:** kNN smoke returns relevant chunks; avg cosine ≥ 0.6.

### Phase 11 — Agent & API

**Inputs:** Retriever, LLM key (in KV), prompt rules.
**Actions:** Implement LangGraph agent (*§3.5*); FastAPI (*§3.6*, *§32*). Containerize:

```dockerfile
# docker/api/Dockerfile
FROM python:3.11-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY src ./src
CMD ["python","-m","uvicorn","src.api:app","--host","0.0.0.0","--port","8080"]
```

**Outputs:** `niloomid/ai-api` image; `/qa` endpoint.
**Validation:** p95 ≤ 2.5s; citations included.

### Phase 12 — DLT Pipeline (Continuous)

**Inputs:** `dlt/pipeline.json`, notebooks.
**Actions:** Import and start DLT (*§4*, *§18*).
**Outputs:** Continuous Bronze→Silver→Gold flow with expectations.
**Validation:** DLT UI shows expectations hits/drops; no SLA breaches.

### Phase 13 — Airflow Orchestration

**Inputs:** Airflow provider, Databricks connection.
**Actions:** Bring up Airflow with Docker Compose; deploy DAG (*§31*).
**Outputs:** Daily DAG runs: `silver → gold → embed`.
**Validation:** All tasks green; SLA email alerts wired.

### Phase 14 — CI/CD & Promotion Gates

**Inputs:** `.github/workflows/ci.yml` (*§6*, *§19*).
**Actions:** Enable Lint, Pytest, GE suites, secret scan, CodeQL, Bundles deploy; environment rules for prod.
**Outputs:** Green checks; dev→prod promotion by approval.
**Validation:** All gates pass; rollback tested.

### Phase 15 — Observability & SRE

**Inputs:** OTel exporter; metrics spec.
**Actions:** Emit structured logs/metrics/traces; dashboards for throughput/lag/GE pass/LLM cost; alerts on thresholds.
**Outputs:** Dashboards + alerts.
**Validation:** Synthetic checks green; on‑call rota active.

### Phase 16 — RAG Evaluation & Judgement

**Inputs:** Labeled eval set.
**Actions:** Run offline harness (*§26*); enable online judge & circuit breakers.
**Outputs:** `ops.rag_eval_metrics`.
**Validation:** hit\_rate ≥ 0.85; faithfulness ≥ 0.75; no regressions.

### Phase 17 — Go‑Live & Runbooks

**Inputs:** All above.
**Actions:** 10% canary; monitor 24h; publish runbooks (*§8*, *§22*); finalize SRE ownership.
**Outputs:** 100% prod traffic; rollback plan stored.
**Validation:** Cost & SLO steady; drills passed.

---

## 1) High‑Level Architecture (HLA)

```mermaid
flowchart LR
  %% --- Sources ---
  subgraph Sources
    s3["S3 / ADLS / HTTP"]
    db["OLTP / DB"]
    docs["Docs / PDF / Web"]
    kafka["Kafka"]
  end

  %% --- Lakehouse ---
  subgraph Lakehouse["Databricks + Delta + Unity Catalog"]
    subgraph Bronze["Bronze"]
      auto["Auto Loader / Batch Landing"]
    end
    subgraph Silver["Silver"]
      clean["Cleanse & Conform"]
      dq["Great Expectations"]
    end
    subgraph Gold["Gold"]
      kpi["KPI / Features"]
      text["Curated Text"]
    end
  end

  %% --- AI / Agentic ---
  subgraph AI["RAG + Agentic"]
    embed["Embeddings"]
    vdb["Vector DB (FAISS · Qdrant)"]
    retr["Retriever"]
    llm["LLM / Prompt Layer"]
    agent["LangGraph Agent"]
    api["FastAPI Service"]
  end

  %% --- Ops ---
  subgraph Ops["CI/CD & Observability"]
    ci["GitHub Actions · Azure Pipelines"]
    mon["OTel · Logs · Metrics"]
    sec["Key Vault · RBAC · Policies"]
  end

  %% --- Edges ---
  s3 --> auto
  db --> auto
  docs --> auto
  kafka --> auto

  auto --> clean --> dq --> kpi
  clean --> text

  text --> embed --> vdb --> retr --> llm --> agent --> api

  ci -.-> Lakehouse
  ci -.-> AI
  mon -.-> AI
  sec -.-> AI
```

**Notes**

* **Unity Catalog** isolates catalogs per environment; RBAC enforces least privilege.
* **Private Endpoints** to Storage; secret scopes backed by **Azure Key Vault**.
* Observability with **OpenTelemetry** traces, **Prometheus/Grafana** dashboards.

---

## 2) Repository Layout (Recommended)

```
ai-pipeline/
├── dags/                          # Airflow DAGs (end-to-end)
│   └── rag_pipeline.py
├── notebooks/                     # Databricks notebooks (DLT + SQL/Py)
│   ├── 00_setup_uc.sql
│   ├── 10_autoloader_bronze.py
│   ├── 20_silver_cleaning.py
│   └── 30_gold_kpis.sql
├── src/
│   ├── ingestion.py               # S3/HTTP/ADLS landing
│   ├── validation.py              # Great Expectations suite wrappers
│   ├── preprocessing.py           # text clean, chunk, metadata
│   ├── embed.py                   # embeddings + FAISS/Qdrant helpers
│   ├── rag.py                     # retriever/QA chains
│   ├── agent.py                   # LangGraph agent & tools
│   ├── api.py                     # FastAPI service
│   └── utils.py                   # logging, config, retries
├── dlt/
│   └── pipeline.json              # DLT pipeline skeleton
├── infra/
│   ├── terraform/                 # Databricks, Storage, VNets, Key Vault
│   └── bicep/                     # (optional) Azure native templates
├── .github/workflows/ci.yml       # or azure-pipelines.yml
├── ge/                            # Great Expectations (context + suites)
├── tests/                         # pytest suites (unit + e2e)
├── docker/
│   ├── airflow/Dockerfile
│   ├── api/Dockerfile
│   └── vector_db/Dockerfile
├── requirements.txt
├── docker-compose.yml
└── README.md
```

---

## 3) Low‑Level Design (LLD): Data & AI Pipelines

### 3.1 Ingestion

* **Batch**: S3/ADLS/HTTP → Bronze via `src/ingestion.py` with retries, idempotent writes
* **Streaming**: Kafka → Bronze Autoloader with 2h watermark for joins

**Bronze Notebook (`10_autoloader_bronze.py`)**

```python
from pyspark.sql.functions import col, input_file_name, current_timestamp
spark.conf.set("cloudFiles.inferColumnTypes", "true")
(spark.readStream
  .format("cloudFiles")
  .option("cloudFiles.format", "json")
  .option("cloudFiles.schemaLocation", "/mnt/lake/_schemas/events")
  .option("cloudFiles.maxFilesPerTrigger", 1000)
  .load("/mnt/lake/landing/events/")
  .withColumn("src_file", input_file_name())
  .withColumn("ingest_ts", current_timestamp())
  .writeStream
  .option("checkpointLocation", "/mnt/lake/_chk/bronze/events")
  .toTable("niloomid_ai.raw.events_bronze"))
```

### 3.2 Silver (Cleanse/Conform)

* Dedup, null/PII handling, type conformance
* **DQ Gate**: Great Expectations suite must pass → otherwise quarantine & alert

**Silver Notebook (`20_silver_cleaning.py`)**

```python
from pyspark.sql import functions as F
src = spark.table("niloomid_ai.raw.events_bronze")
clean = (src.dropDuplicates(["event_id"])
            .filter("event_type is not null")
            .withColumn("event_dt", F.to_date("event_ts")))
clean.write.mode("overwrite").saveAsTable("niloomid_ai.clean.events_silver")
```

**Great Expectations (example suite)** — `ge/suites/events_silver.json`

```json
{
  "expectations": [
    {"expectation_type": "expect_column_values_to_not_be_null", "kwargs": {"column": "event_type"}},
    {"expectation_type": "expect_column_values_to_match_regex", "kwargs": {"column": "event_id", "regex": "^[A-Z0-9_-]{12,}$"}},
    {"expectation_type": "expect_table_row_count_to_be_between", "kwargs": {"min_value": 1}}
  ]
}
```

### 3.3 Gold (KPIs/Curated Text)

* Aggregate KPIs for dashboards; build **curated text** table for RAG

**Gold SQL (`30_gold_kpis.sql`)**

```sql
CREATE TABLE IF NOT EXISTS niloomid_ai.gold.kpi_daily AS
SELECT event_dt, event_type, COUNT(*) AS cnt
FROM niloomid_ai.clean.events_silver
GROUP BY event_dt, event_type;

CREATE TABLE IF NOT EXISTS niloomid_ai.gold.docs_text AS
SELECT doc_id,
       CONCAT_WS('\n', COLLECT_LIST(chunk_text)) AS full_text
FROM niloomid_ai.clean.docs_chunks
GROUP BY doc_id;
```

### 3.4 RAG & Vector Indexing

**Preprocess & Chunk (`src/preprocessing.py`)**

```python
import pandas as pd, re

def clean_text(t: str) -> str:
    t = re.sub(r"\s+", " ", t or "").replace("\u200b", "").strip()
    return t

def split_documents(df: pd.DataFrame, chunk_size: int = 512, overlap: int = 50):
    chunks = []
    for _, r in df.iterrows():
        words = (r["content"] or "").split()
        for i in range(0, len(words), chunk_size - overlap):
            chunks.append({"doc_id": r["doc_id"],
                           "chunk_id": f"{r['doc_id']}_{i}",
                           "chunk_text": clean_text(" ".join(words[i:i+chunk_size]))})
    return pd.DataFrame(chunks)
```

**Embeddings + FAISS (`src/embed.py`)**

```python
from sentence_transformers import SentenceTransformer
import numpy as np, faiss
model = SentenceTransformer('all-MiniLM-L6-v2')

def embed_texts(texts):
    vecs = model.encode(texts, normalize_embeddings=True)
    return np.array(vecs)

def build_index(vecs):
    idx = faiss.IndexFlatIP(vecs.shape[1])
    idx.add(vecs)
    return idx
```

**Retriever + QA (`src/rag.py`)**

```python
from langchain.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

def make_qa(retriever, llm):
    prompt = PromptTemplate(
      input_variables=["context","question"],
      template=("Use ONLY the context to answer.\nContext:\n{context}\nQ: {question}\nA:")
    )
    return RetrievalQA.from_chain_type(llm=llm, retriever=retriever, chain_type="stuff", chain_type_kwargs={"prompt": prompt})
```

### 3.5 Agentic Workflow (LangGraph)

**Agent (`src/agent.py`)**

```python
from langgraph.graph import StateGraph
from typing import Dict

class St(Dict): ...

g = StateGraph(St)

def tool_search(state):
    # call retriever; add results into state["context"]
    return state

def tool_answer(state):
    # call LLM with prompt
    return state

g.add_node("search", tool_search)

g.add_node("answer", tool_answer)

g.add_edge("search", "answer")
app = g.compile()
```

### 3.6 API Layer (FastAPI)

**Service (`src/api.py`)**

```python
from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI(title="NILOOMID AI API")

class Q(BaseModel):
    question: str

@app.post("/qa")
def qa(q: Q):
    # 1) retrieve via FAISS  2) LLM answer  3) return
    return {"answer": "stub"}
```

---

## 4) Delta Live Tables (DLT) — Dataflow

```mermaid
flowchart LR
  A[Autoloader: landing/events] --> B((Bronze events_bronze))
  B --> C[GE Gate]
  C --> D((Silver events_silver))
  D --> E{Branch}
  E -->|KPIs| F((Gold kpi_daily))
  E -->|Text| G((Gold docs_text))
  G --> H[Embeddings]
  H --> I[FAISS/Qdrant]
  I --> J[Retriever → LLM → Agent → API]
```

**DLT `pipeline.json`**

```json
{
  "name": "niloomid-dlt",
  "edition": "ADVANCED",
  "clusters": [{"num_workers": 2}],
  "libraries": [],
  "continuous": true,
  "development": true,
  "photon": true
}
```

---

## 5) Orchestration — Airflow DAG

```python
# dags/rag_pipeline.py
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime

with DAG("rag_pipeline", start_date=datetime(2025,1,1), schedule_interval="@daily", catchup=False) as dag:
    ingest = PythonOperator(task_id="ingest", python_callable=lambda: None)
    validate = PythonOperator(task_id="validate", python_callable=lambda: None)
    silver = PythonOperator(task_id="to_silver", python_callable=lambda: None)
    gold = PythonOperator(task_id="to_gold", python_callable=lambda: None)
    index = PythonOperator(task_id="build_index", python_callable=lambda: None)
    serve = PythonOperator(task_id="deploy_api", python_callable=lambda: None)

    ingest >> validate >> silver >> gold >> index >> serve
```

**Watermarks & Joins**: Use 2h watermark for stream‑batch joins in Silver to avoid late data skew.

---

## 6) CI/CD — Tests, Quality Gates, Deploy

```mermaid
flowchart LR
  C[Commit/PR] --> Lint[Lint + Type Check]
  Lint --> Py[Pytest]
  Py --> GE[GE Suites]
  GE --> Sec[SAST/Secrets scan]
  Sec --> Pack[Build Docker + DLT cfg]
  Pack --> Plan[Terraform Plan]
  Plan --> Apply[Deploy Dev]
  Apply --> Smoke[Smoke Tests]
  Smoke --> Promote{Promote?}
  Promote -->|Yes| Prod[Deploy Prod]
  Promote -->|No| Fix[Fail & Rollback]
```

**GitHub Actions (`.github/workflows/ci.yml`)**

```yaml
name: ci
on: [push, pull_request]
jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with: {python-version: '3.11'}
      - run: pip install -r requirements.txt
      - run: pytest -q
      - run: echo "Run GE suites here"
      - run: docker build -t niloomid/api ./docker/api
```

---

## 7) Security, Governance, & Lineage

* **Unity Catalog** for data governance & access policies
* **Key Vault** secret scopes; no plaintext keys in code/CI
* **Lineage** via Unity Catalog + Delta history; log run IDs, input/output tables
* **PII**: Hashing/tokenization in Silver; role‑based masking views in Gold

---

## 8) Observability & Runbooks

* **Logging**: Structured logs (JSON) with correlation IDs from DAG → notebooks → API
* **Metrics**: Throughput, lag, error rate, GE pass %, top‑k recall, LLM token usage
* **Tracing**: OTel spans around retriever/LLM calls; propagate request IDs
* **Alerts**: Slack/Email on GE failures, 5xx spikes, cost anomalies

**Runbooks**

* Data quality failure → quarantine partition, open incident, backfill
* Model drift → lower confidence, trigger re‑embed + re‑index
* Cost spike → autoscaling policy review, cache thresholds, batch window tuning

---

## 9) Example Tests (pytest)

```python
# tests/test_chunks.py
from src.preprocessing import split_documents
import pandas as pd

def test_split_documents_basic():
    df = pd.DataFrame([{"doc_id":"A1","content":"one two three four five six"}])
    out = split_documents(df, chunk_size=3, overlap=1)
    assert len(out) >= 2
    assert set(out.columns) == {"doc_id","chunk_id","chunk_text"}
```

---

## 10) Deployment Guide (Step‑by‑Step)

1. **Infra**: Deploy Databricks workspace, Storage, VNets, Key Vault (Terraform/Bicep)
2. **Unity Catalog**: Create catalog/schemas + RBAC; mount ADLS (MI)
3. **Secrets**: Create secret scopes for LLM/DB creds
4. **Data**: Configure Autoloader paths; land sample JSON/CSV
5. **DLT**: Import `pipeline.json`, attach notebooks, start continuous mode
6. **GE**: Initialize context; run suites on Silver before Gold writes
7. **RAG**: Run preprocessing → embeddings → FAISS/Qdrant index build
8. **Agent/API**: `uvicorn src.api:app --host 0.0.0.0 --port 8080`
9. **Orchestration**: Enable Airflow DAG; set SLA and alert rules
10. **CI/CD**: Protect main; require tests + GE; enable environment promotion

---

## 11) Appendix — Metadata Tables (Optional, aligns with UDP/ETL frameworks)

**Batch / Job / Proc / Proc‑Param (illustrative DDL)**

```sql
CREATE TABLE IF NOT EXISTS meta.batch (
  batch_nm STRING, framework STRING, dag_nm STRING, schedule STRING
);
CREATE TABLE IF NOT EXISTS meta.job (
  job_nm STRING, batch_nm STRING, layer STRING, module STRING, class STRING
);
CREATE TABLE IF NOT EXISTS meta.proc (
  proc_nm STRING, job_nm STRING, lower_bound_ts TIMESTAMP, high_val_ts TIMESTAMP
);
CREATE TABLE IF NOT EXISTS meta.proc_param (
  proc_nm STRING, parm_key STRING, parm_val STRING
);
```

---

## 12) Traceability Map (What feeds what)

* **Bronze → Silver**: `raw.events_bronze` → `clean.events_silver` (GE gate)
* **Silver → Gold**: `clean.events_silver` → `gold.kpi_daily`, `gold.docs_text`
* **Gold/Text → Vector DB**: `gold.docs_text` → embeddings → FAISS/Qdrant index
* **Vector DB → API**: retriever → LLM → agent → FastAPI (served)

---

## 13) Ready‑to‑Use Checklists

**Pre‑Prod**

* [ ] Unity Catalog RBAC; secrets mounted
* [ ] DLT pipeline green ≥ 24h
* [ ] GE suites ≥ 99% pass at Silver; schema registry stable
* [ ] CI gates: lint, tests, GE, SAST, secret scan

**Go‑Live**

* [ ] Canary 10% traffic; monitor p95 latency
* [ ] Cost guardrails; autoscaling verified
* [ ] On‑call rota and runbooks published

---

## 14) Environment, Naming & Conventions (Validated)

**Workspaces & UC**

* Workspaces: `niloomid-{dev|test|prod}`; Resource Groups: `rg-niloomid-{env}`.
* Unity Catalog objects:

  * Catalogs: `niloomid_{env}` (e.g., `niloomid_dev`).
  * Schemas: `raw`, `clean`, `gold`, `meta`, `ops`.
  * Tables follow `{domain}_{entity}_{layer}` e.g., `events_bronze`, `events_silver`, `kpi_daily`.
* Jobs & DAGs: `RAG_{domain}_{env}`; Clusters: `dbrx-{layer}-{env}`.

**RBAC**

* Roles: `de_admin`, `de_pipeline`, `data_analyst`, `secops`.
* Minimal grants (examples):

  * `GRANT USE CATALOG ON CATALOG niloomid_{env} TO de_admin, de_pipeline, data_analyst;`
  * `GRANT SELECT ON SCHEMA niloomid_{env}.gold TO data_analyst;`
  * Row‑/column‑level masking via views (see §17.3).

---

## 15) Cluster Policies (Security & Cost)

**Policy JSON (example)**

```json
{
  "spark_version": {"type": "fixed", "value": "14.3.x-scala2.12"},
  "autotermination_minutes": {"type": "range", "minValue": 10, "maxValue": 120, "defaultValue": 30},
  "num_workers": {"type": "range", "minValue": 1, "maxValue": 10, "defaultValue": 2},
  "data_security_mode": {"type": "fixed", "value": "SINGLE_USER"},
  "runtime_engine": {"type": "fixed", "value": "PHOTON"},
  "aws_attributes": {"availability": {"type": "fixed", "value": "SPOT_WITH_FALLBACK"}},
  "azure_attributes": {"first_on_demand": {"type": "fixed", "value": 1}},
  "custom_tags": {"CostCenter": "NILOOMID-DE", "Owner": "DataPlatform"}
}
```

Attach to all jobs; enforce spot-with-fallback (or Azure low‑priority) with on‑demand minimum.

---

## 16) Data Contracts (Schema, SLAs, DQ)

**Contract YAML (events) — `contracts/events.yml`**

```yaml
name: events
owner: ai-platform@niloomid.com
sla:
  freshness: 15m
  availability: 99.5%
schema:
  event_id: {type: string, required: true, regex: "^[A-Z0-9_-]{12,}$"}
  event_ts:  {type: timestamp, required: true}
  event_type:{type: string, required: true, allowed: [CLICK, VIEW, ERROR]}
  content:   {type: string, required: false}
quality_gates:
  - non_null: [event_id, event_ts, event_type]
  - unique: [event_id]
  - row_count_min: 1
retention:
  bronze: {mode: days, value: 7}
  silver: {mode: months, value: 12}
privacy:
  pii_columns: [content]
  policy: redact
```

**Enforcement**: Validate contracts in CI (schema diff), and at runtime via GE suite mapping.

---

## 17) DQ, Privacy & Masking (Operationalized)

**17.1 Great Expectations (suite as YAML)**

```yaml
expectations:
  - expect_column_values_to_not_be_null: {column: event_type}
  - expect_column_values_to_match_regex: {column: event_id, regex: "^[A-Z0-9_-]{12,}$"}
  - expect_table_row_count_to_be_between: {min_value: 1}
```

**17.2 Quarantine & Backfill**

* On GE failure: write failing rows to `niloomid_{env}.ops.quarantine_events` with run\_id & suite.
* Open incident, page on‑call, and execute backfill notebook with partition filters.

**17.3 Masking View (Gold)**

```sql
CREATE OR REPLACE VIEW niloomid_{env}.gold.events_masked AS
SELECT event_id,
       event_ts,
       event_type,
       CASE WHEN is_member('data_analyst_pii') THEN content ELSE substr(sha2(content,256),1,16) END AS content
FROM niloomid_{env}.clean.events_silver;
```

---

## 18) DLT Tables with Expectations (Enforced)

**DLT notebook snippet**

```python
import dlt
from pyspark.sql.functions import col

@dlt.table(name="events_bronze")
def bronze():
    return spark.readStream.format("cloudFiles").option("cloudFiles.format","json").load("/mnt/lake/landing/events")

@dlt.expect("valid_event_type", "event_type IS NOT NULL")
@dlt.expect_or_drop("valid_id", "event_id RLIKE '^[A-Z0-9_-]{12,}$'")
@dlt.table(name="events_silver")
def silver():
    return dlt.read_stream("events_bronze").dropDuplicates(["event_id"]).withColumn("event_dt", col("event_ts").cast("date"))

@dlt.table(name="kpi_daily")
def kpi():
    return dlt.read("events_silver").groupBy("event_dt","event_type").count()
```

---

## 19) CI/CD — Advanced (Bundles, Scans, Promotion)

**Databricks Asset Bundles (DAB) — `databricks.yml`**

```yaml
bundle:
  name: niloomid-ai
workspace:
  root_path: "/Shared/niloomid-ai"
resources:
  jobs:
    rag-pipeline:
      name: RAG Pipeline
      tasks:
        - task_key: silver
          notebook_task: {notebook_path: "/Repos/niloomid/20_silver_cleaning.py"}
      schedule: {quartz_cron_expression: "0 0 * * * ?"}
targets:
  dev: {workspace: {host: ${env.DBRKS_HOST}}, default: true}
  prod: {workspace: {host: ${env.DBRKS_HOST_PROD}}}
```

**GitHub Actions — hardened**

```yaml
- name: Secret scan
  uses: trufflesecurity/trufflehog@v3
- name: SAST
  uses: github/codeql-action/init@v3
  with: {languages: python}
- name: Deploy Bundles (dev)
  run: |
    pip install databricks-sdk databricks-cli dbx
    databricks bundles deploy -t dev
```

**Promotion Gate**

* Require: unit+integration tests green, GE pass ≥ 99%, cost budget OK, drift < threshold, p95 latency ≤ 2.5s on canary.

---

## 20) Networking & Secrets

* **Private Link** / service endpoints for Storage & Databricks control plane.
* **Key Vault** backed secret scopes: `kv-llm-key`, `kv-faiss`, `kv-azure-openai`.
* No PATs in CI; use OIDC‑based federation to Databricks & Azure.

---

## 21) RAG Evaluation Harness (Validated)

```python
# tests/test_rag_eval.py
from src.rag import build_qa
import numpy as np

def precision_at_k(retrieved, relevant, k=5):
    return len(set(retrieved[:k]) & set(relevant)) / max(k,1)

def test_eval_sample():
    # pretend ids
    retrieved = ["d1","d2","d3","d4","d5"]
    relevant = ["d2","d9","d5"]
    p5 = precision_at_k(retrieved, relevant, 5)
    assert 0.0 <= p5 <= 1.0
```

**Metrics to track**: retrieval hit‑rate, precision\@k, faithfulness (LLM judge), answer latency, token usage, cost/request.

---

## 22) Incident Response & SRE Playbook

* **Sev1**: pipeline down or PII leak suspected → freeze writes, rotate keys, incident bridge.
* **Sev2**: GE failure > 30m → quarantine + backfill; RCA within 24h.
* **Sev3**: KPI drift → review transformations; schedule re‑embed.

---

## 23) Source Mapping — How Repo Files Were Incorporated

* `README.md`: baseline blueprint sections (repo layout, UC setup, ingestion, GE, RAG, Airflow, CI) → **sections 1–13**.
* `1.md`, `2.md`, `3.md`, `5.md`, `a.md`, `final1.md`, `md.md`: merged into **HLA/LLD**, **DLT/GE**, **RAG/Agent**, and **CI/CD** steps; duplicated items deduped; gaps filled (policies, contracts, evaluation, SRE).
* Any diagrams referenced were re‑drawn as Mermaid for reproducibility.

---

## 24) Full Validation Checklist (Pass/Fail with Evidence)

**Env & UC**

* [ ] Catalogs/schemas exist; RBAC grants logged (screenshot or SQL history link)
* [ ] Private Link enabled; subnets isolated

**Pipelines**

* [ ] Bronze Autoloader active ≥ 2h; watermark 2h; lag < 5m
* [ ] Silver GE pass ≥ 99%; quarantine empty
* [ ] Gold KPIs populated; docs\_text non‑empty

**RAG/Agent/API**

* [ ] Index contains ≥ N vectors; cosine sim average ≥ 0.6 on sample
* [ ] p95 latency ≤ 2.5s on canary; 0 error spikes in last 24h

**CI/CD & Security**

* [ ] CI green (tests, lint, SAST, secret scan)
* [ ] DAB deploy succeeded; Workflows scheduled
* [ ] Cost dashboard within budget

---

## 25) RAG Flow (Detailed & Validated)

```mermaid
sequenceDiagram
  autonumber
  participant U as User
  participant API as FastAPI
  participant AG as Agent (LangGraph)
  participant RT as Retriever (Hybrid)
  participant RR as Reranker
  participant LLM as LLM
  participant JDG as Judge (LLM-as-judge)

  U->>API: POST /qa {question}
  API->>AG: build state (trace_id, user_id)
  AG->>RT: retrieve top_k (FAISS + BM25)
  RT-->>AG: candidates (docs + scores)
  AG->>RR: cross-encoder rerank
  RR-->>AG: reranked top_k
  AG->>LLM: grounded prompt (context, rules)
  LLM-->>AG: answer + citations
  AG->>JDG: evaluate (faithfulness, relevance)
  JDG-->>AG: metrics + pass/fail
  AG->>API: response (answer, citations, metrics)
  API-->>U: JSON (answer, sources, eval)
```

**Hybrid Retrieval**

* Vector search (FAISS/Qdrant) + lexical BM25 (Elastic/OpenSearch or `rank_bm25`) → union → rerank (cross‑encoder) → top‑k.
* Document chunking: 512–1024 tokens with 10–50 overlap; normalize whitespace; strip boilerplate; attach metadata (doc\_id, section, timestamp, source).

**Grounding & Prompt Rules**

* Always include **system instructions**: “Answer strictly from context. If insufficient, say ‘I don’t know.’ Return citations as `[doc_id:chunk_id]`.”
* Inject **guardrails** (PII redaction, safe completion) and **domain glossary** to reduce ambiguity.

---

## 26) RAG Judgement & Evaluation (Automated)

**Metrics**: `retrieval_hit_rate`, `precision@k`, `faithfulness`, `answer_relevance`, `latency_p95`, `cost_per_req`.

**Eval Harness (offline)** — `tests/test_rag_faithfulness.py`

```python
from typing import List
from src.rag import make_qa

# stub LLM & retriever would be injected in real tests

def faithfulness_score(answer: str, context: List[str]) -> float:
    # heuristic: penalize unsupported claims
    ctx = " ".join(context).lower()
    unsupported = sum(1 for sent in answer.split('.') if sent and sent.lower() not in ctx)
    total = max(1, len(answer.split('.')))
    return max(0.0, 1.0 - unsupported/total)

def test_faithfulness_basic():
    ans = "A is part of B. C is unrelated."
    ctx = ["A is part of B"]
    s = faithfulness_score(ans, ctx)
    assert 0.0 <= s <= 1.0
```

**Online Eval**

* Log per‑request: retrieved\_ids, rerank\_scores, token\_usage, latency, judge\_scores.
* Canary gating: if `faithfulness < 0.7` or `hit_rate < 0.8`, trip circuit → fallback (template reply or escalate to human‑in‑loop).

---

## 27) Unity Catalog Setup — Commands (Azure)

> Run in a UC‑enabled SQL warehouse or Databricks SQL.

```sql
-- 1) Storage credential via managed identity (example)
CREATE STORAGE CREDENTIAL sc_niloomid
  WITH AZURE_MANAGED_IDENTITY
  COMMENT 'MI for lake access';

-- 2) External location for landing/raw
CREATE EXTERNAL LOCATION loc_lake_landing
  URL 'abfss://landing@<storage_account>.dfs.core.windows.net/'
  WITH STORAGE CREDENTIAL sc_niloomid
  COMMENT 'Landing zone';

-- 3) Catalogs per environment
CREATE CATALOG IF NOT EXISTS niloomid_dev;
CREATE CATALOG IF NOT EXISTS niloomid_test;
CREATE CATALOG IF NOT EXISTS niloomid_prod;

-- 4) Schemas (databases)
CREATE SCHEMA IF NOT EXISTS niloomid_dev.raw;
CREATE SCHEMA IF NOT EXISTS niloomid_dev.clean;
CREATE SCHEMA IF NOT EXISTS niloomid_dev.gold;
CREATE SCHEMA IF NOT EXISTS niloomid_dev.meta;
CREATE SCHEMA IF NOT EXISTS niloomid_dev.ops;

-- 5) Volumes for unstructured content
CREATE VOLUME IF NOT EXISTS niloomid_dev.raw.docs VOLATILE;

-- 6) Grants (principals = groups/SPNs)
GRANT USE CATALOG ON CATALOG niloomid_dev TO `de_admin`, `de_pipeline`, `data_analyst`;
GRANT SELECT ON SCHEMA niloomid_dev.gold TO `data_analyst`;
GRANT MODIFY, SELECT ON SCHEMA niloomid_dev.clean TO `de_pipeline`;
```

**Best Practices**

* Use **external locations** for Bronze/landing; **managed tables** for Silver/Gold.
* Enforce **constraints** and **table properties** (see §29 DDL) and enable change data feed if required.
* Prefer **service principals** mapped to UC groups; avoid user PATs for pipelines.

---

## 28) Security Best Practices (UC + Network)

* **Identity & Access**: UC groups for roles; service principals for pipelines; **least privilege**.
* **Secrets**: Key Vault–backed secret scopes; no secrets in notebooks/CI logs.
* **Network**: Private Link/Service Endpoints to Storage; restrict egress; IP access lists.
* **Compute**: Single‑user mode for production jobs; cluster policies enforcing Photon, auto‑termination, tags; pinned runtimes.
* **Data**: Dynamic views for row/column masking; PII redaction at Silver; DLP scanning in CI.
* **Lineage/Audit**: UC lineage + Delta history; log `run_id`, inputs/outputs per task.

**Dynamic Row Filter (example)**

```sql
CREATE OR REPLACE VIEW niloomid_dev.clean.events_rls AS
SELECT * FROM niloomid_dev.clean.events_silver
WHERE CASE
  WHEN current_user() IN ('analyst_apac') THEN region = 'APAC'
  WHEN current_user() IN ('analyst_eu')   THEN region = 'EU'
  ELSE true END;
```

---

## 29) Data Model DDL (Bronze → Silver → Gold)

```sql
-- Bronze
CREATE TABLE IF NOT EXISTS niloomid_dev.raw.events_bronze (
  event_id STRING,
  event_ts TIMESTAMP,
  event_type STRING,
  content STRING,
  src_file STRING,
  ingest_ts TIMESTAMP
) USING DELTA TBLPROPERTIES (
  delta.autoOptimize.optimizeWrite = true,
  delta.autoOptimize.autoCompact = true
);

-- Silver (constraints + expectations mirrored)
CREATE TABLE IF NOT EXISTS niloomid_dev.clean.events_silver (
  event_id STRING NOT NULL,
  event_ts TIMESTAMP NOT NULL,
  event_type STRING NOT NULL,
  content STRING,
  event_dt DATE GENERATED ALWAYS AS (CAST(event_ts AS DATE))
) USING DELTA TBLPROPERTIES (
  delta.enableChangeDataFeed = true,
  delta.constraints.event_id_chk = 'event_id RLIKE "^[A-Z0-9_-]{12,}$"'
);

-- Gold KPIs
CREATE TABLE IF NOT EXISTS niloomid_dev.gold.kpi_daily AS
SELECT event_dt, event_type, COUNT(*) AS cnt
FROM niloomid_dev.clean.events_silver
GROUP BY event_dt, event_type;

-- Docs & chunks for RAG
CREATE TABLE IF NOT EXISTS niloomid_dev.clean.docs_raw (
  doc_id STRING,
  source STRING,
  content STRING,
  load_ts TIMESTAMP
) USING DELTA;

CREATE TABLE IF NOT EXISTS niloomid_dev.clean.docs_chunks (
  doc_id STRING,
  chunk_id STRING,
  chunk_text STRING
) USING DELTA;
```

---

## 30) Pipelines — DLT + Jobs (Validated)

**DLT expectations** (see §18) enforce schema and drop bad rows. Enable continuous mode.

**Databricks Workflow (JSON)** — daily rebuild of KPIs & index

```json
{
  "name": "niloomid-gold-refresh",
  "tasks": [
    {"task_key": "silver",
     "notebook_task": {"notebook_path": "/Repos/niloomid/20_silver_cleaning.py"}},
    {"task_key": "gold",
     "notebook_task": {"notebook_path": "/Repos/niloomid/30_gold_kpis.sql"},
     "depends_on": [{"task_key": "silver"}]},
    {"task_key": "embed",
     "notebook_task": {"notebook_path": "/Repos/niloomid/embed_index.py"},
     "depends_on": [{"task_key": "gold"}]}
  ]
}
```

---

## 31) Airflow Setup (Docker + Databricks Provider)

**Connections**

* `databricks_default`: host/workspace, OAuth or PAT (prefer OAuth via OIDC).
* `niloomid_kv`: for pulling non-DBX secrets if absolutely needed.

**DAG** — `dags/rag_pipeline.py`

```python
from airflow import DAG
from airflow.providers.databricks.operators.databricks import DatabricksSubmitRunOperator
from datetime import datetime

new_cluster = {
  "spark_version": "14.3.x-scala2.12",
  "num_workers": 2,
  "data_security_mode": "SINGLE_USER",
  "spark_conf": {"spark.databricks.delta.properties.defaults.checkpointInterval": "10"}
}

with DAG("rag_pipeline", start_date=datetime(2025,1,1), schedule_interval="@daily", catchup=False) as dag:
    silver = DatabricksSubmitRunOperator(
        task_id="silver",
        json={"new_cluster": new_cluster,
              "notebook_task": {"notebook_path": "/Repos/niloomid/20_silver_cleaning.py"}}
    )
    gold = DatabricksSubmitRunOperator(
        task_id="gold",
        json={"new_cluster": new_cluster,
              "notebook_task": {"notebook_path": "/Repos/niloomid/30_gold_kpis.sql"}}
    )
    embed = DatabricksSubmitRunOperator(
        task_id="embed",
        json={"new_cluster": new_cluster,
              "notebook_task": {"notebook_path": "/Repos/niloomid/embed_index.py"}}
    )
    silver >> gold >> embed
```

---

## 32) API & Agent (Hardened)

**Prompting**

* System: “You are an enterprise assistant. Use only supplied context. If missing, say ‘I don’t know.’ Return citations.”
* Policy snippets: blocked topics/PII; max answer length; cite top‑3 contexts.

**FastAPI (with tracing & limits)**

```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import time

app = FastAPI()

class Q(BaseModel):
    question: str

@app.post("/qa")
def qa(q: Q):
    t0 = time.time()
    # retrieve → rerank → llm → judge (omitted)
    latency = time.time() - t0
    if latency > 2.5:  # p95 guardrail sample
        raise HTTPException(503, "SLA breach")
    return {"answer": "stub", "latency": latency, "citations": []}
```

---

## 33) Validation Evidence — What to Capture

* **Screenshots/links**: UC grants, lineage graph, DLT run with expectations, Airflow DAG runs.
* **Tables**: metrics table `ops.rag_eval_metrics` with daily aggregates.
* **Logs**: structured request logs with `trace_id`, token usage, latency.

---

## 34) Parameterization Matrix

| Layer     | Parameters                                             |
| --------- | ------------------------------------------------------ |
| Ingestion | landing path, schema registry path, maxFilesPerTrigger |
| Silver    | GE suite name, quarantine path/table, primary keys     |
| Gold      | KPI definitions, partition columns                     |
| RAG       | chunk\_size, overlap, top\_k, reranker\_model          |
| API       | max\_tokens, timeout\_s, p95\_budget\_s                |
| CI/CD     | branches, env targets, promotion rules                 |

---

**End of Blueprint**
