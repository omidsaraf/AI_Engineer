# GitHub Repository – Optus Sydney Databricks Lakehouse

This is a complete, production‑ready scaffold you can paste into a new GitHub repo. It includes **HLA/LLD docs**, **Unity Catalog SQL**, **DLT/Jobs**, **Airflow DAGs**, **RAG/LLM/Agents**, **CI/CD (GitHub Actions + Azure DevOps)**, **Terraform**, **tests**, and **Optus best practices**.

> Replace `optus-...` placeholders with your environment values.

---

## 1) Repository Structure

```text
optus-sydney-databricks-lakehouse/
├─ README.md
├─ LICENSE
├─ CONTRIBUTING.md
├─ SECURITY.md
├─ CODEOWNERS
├─ .gitignore
├─ .editorconfig
├─ .pre-commit-config.yaml
├─ .gitattributes
├─ azure-pipelines.yml                     # optional: if using Azure DevOps CI/CD
├─ devops/                                 # Azure DevOps multi-stage (alt to GH Actions)
│  ├─ ci.yml
│  └─ cd.yml
├─ .github/
│  ├─ ISSUE_TEMPLATE.md
│  ├─ PULL_REQUEST_TEMPLATE.md
│  └─ workflows/
│     ├─ ci.yml                            # lint/tests/contracts
│     ├─ rag_eval.yml                      # LLM-judge gates (optional)
│     └─ codeql.yml                        # security scan
├─ docs/
│  ├─ HLA.md
│  ├─ LLD.md
│  ├─ RAG.md
│  ├─ Agents.md
│  ├─ Runbook.md
│  └─ Architecture.mmd                     # Mermaid diagrams
├─ infra/
│  ├─ azure/
│  │  ├─ main.tf
│  │  └─ variables.tf
│  └─ databricks/
│     ├─ main.tf
│     └─ policies/etl_restricted.json
├─ pipelines/
│  ├─ bundles.yaml
│  ├─ dlt/
│  │  └─ telecom_dlt.json
│  └─ jobs/
│     ├─ bronze_autoloader_job.json
│     ├─ silver_standardize_job.json
│     └─ gold_publish_job.json
├─ orchestration/
│  └─ airflow/
│     ├─ dags/
│     │  ├─ domain_ingest_daily.py
│     │  ├─ silver_transform.py
│     │  ├─ gold_publish.py
│     │  ├─ rag_pipeline.py
│     │  └─ agent_proactive_tasks.py
│     └─ plugins/
│        └─ callbacks.py
├─ domains/
│  ├─ jarvis/
│  │  ├─ contracts/jarvis_service.schema.json
│  │  └─ notebooks/
│  │     ├─ bronze_autoloader_jarvis.py
│  │     └─ silver_service_cleanse.py
│  ├─ amdm/
│  │  ├─ contracts/amdm_address.schema.json
│  │  └─ notebooks/
│  │     ├─ bronze_autoloader_amdm.py
│  │     └─ silver_address_standardize.py
│  ├─ pega/
│  │  ├─ contracts/pega_case.schema.json
│  │  └─ notebooks/
│  │     ├─ bronze_autoloader_pega.py
│  │     └─ silver_case_flatten.py
│  └─ cmdm/
│     ├─ contracts/cmdm_customer.schema.json
│     └─ notebooks/
│        ├─ bronze_autoloader_cmdm.py
│        └─ silver_master_survivorship.py
├─ notebooks/
│  ├─ 01_bronze_autoloader.py
│  ├─ 02_silver_cleanse.py
│  ├─ 03_gold_marts.py
│  ├─ 04_quality_expectations.py
│  ├─ 05a_rag_ingest_docs_from_adls.py
│  ├─ 05b_rag_build_cards_from_gold.py
│  ├─ 05c_rag_chunk_and_embed.py
│  ├─ 05d_rag_vector_index.py
│  ├─ 06_rag_retrieve_and_answer.py
│  └─ 07_agent_proactive_summary.py
├─ llm_agents/
│  ├─ agents/optus_assistant.py
│  ├─ serve/router.py
│  └─ tools/
│     ├─ sql_tool.py
│     ├─ retriever_tool.py
│     └─ http_tool.py
├─ quality/
│  ├─ dq_rules.yaml
│  └─ expectations/
├─ sql/
│  ├─ create_uc_objects.sql
│  ├─ bronze_ddls.sql
│  ├─ silver_ddls.sql
│  ├─ gold_merges.sql
│  └─ sfdc_outbound.sql
├─ tests/
│  ├─ unit/
│  │  ├─ test_contracts.py
│  │  └─ test_transformations.py
│  └─ integration/
│     └─ test_end_to_end.py
└─ tools/
   └─ validate_contracts.py
```

---

## 2) Top-level Files (copy into repo)

### README.md

```markdown
# Optus Sydney – Azure Databricks Lakehouse

Production-grade lakehouse for Optus: ADLS + Databricks (UC, DLT, Jobs) + Airflow + RAG/LLM Agents, with CI/CD and Terraform.

## Quickstart
1. **Infra**: `infra/azure` + `infra/databricks` (Terraform) → UC, storage, policies.
2. **UC setup**: run `sql/create_uc_objects.sql` in Databricks SQL.
3. **Pipelines**: `databricks bundles deploy -t dev`; create Airflow connections/variables.
4. **Ingest**: trigger `domain_ingest_daily` DAG; verify Bronze→Silver→Gold.
5. **RAG**: run notebooks `05a..05d`; create Vector index.
6. **Serve**: deploy `llm_agents/serve/router.py` as Model Serving endpoint.

## Docs
- [docs/HLA.md](docs/HLA.md) · [docs/LLD.md](docs/LLD.md) · [docs/RAG.md](docs/RAG.md) · [docs/Agents.md](docs/Agents.md) · [docs/Runbook.md](docs/Runbook.md)

## Environments
`dev` → `test` → `prod` via Bundles targets and gated approvals.
```

### LICENSE

```text
MIT License

Copyright (c) 2025 Optus

Permission is hereby granted, free of charge, to any person obtaining a copy...
```

### CONTRIBUTING.md

```markdown
# Contributing
- Trunk-based development; feature branches; squash merge.
- Commit format: `type(scope): subject` (Conventional Commits).
- All PRs must pass: lint, tests, contract validation, rag-eval (if touching RAG).
```

### SECURITY.md

```markdown
# Security
- No secrets in repo; use Azure Key Vault.
- Report issues to security@optus.com.au.
- Private Link enforced for external services.
```

### CODEOWNERS

```text
* @your-github-handle
infra/* @cloud-owners
orchestration/* @data-platform-owners
llm_agents/* @ai-governance-owners
```

### .gitignore

```text
__pycache__/
*.pyc
*.ipynb_checkpoints
.env
.venv/
.databricks/
**/.idea/
**/.vscode/
.DS_Store
terraform.tfstate*
```

### .pre-commit-config.yaml

```yaml
repos:
  - repo: https://github.com/psf/black
    rev: 24.4.2
    hooks: [{id: black}]
  - repo: https://github.com/pycqa/flake8
    rev: 7.0.0
    hooks: [{id: flake8}]
  - repo: https://github.com/pre-commit/mirrors-prettier
    rev: v4.0.0-alpha.8
    hooks: [{id: prettier}]
```

---

## 3) Docs

### docs/HLA.md

````markdown
# High-Level Architecture
```mermaid
flowchart LR
    A[Sources: SFDC, Jarvis, AMDM/BCC, Pega, CMDM] --> B[Bronze (Delta)]
    B --> C[Silver (Cleansed/Conformed)]
    C --> D[Gold (Marts)]
    C --> V[Vector Search Index]
    V --> R[Retriever]
    D --> S[SQL Tool]
    R --> P[Planner/Router]
    S --> P
    P --> L[LLM]
````

````

### docs/RAG.md
```markdown
# RAG
- Index curated docs + knowledge cards only.
- Vector DB: Databricks Vector Search.
- Evaluation: LLM-as-judge, CI gates.
````

---

## 4) Unity Catalog & DDL

### sql/create\_uc\_objects.sql (excerpt)

```sql
CREATE CATALOG IF NOT EXISTS ep_raw; CREATE CATALOG IF NOT EXISTS ep_curated; CREATE CATALOG IF NOT EXISTS ep_marts; CREATE CATALOG IF NOT EXISTS ep_ai;
-- Schemas per domain
USE CATALOG ep_raw;    CREATE SCHEMA IF NOT EXISTS jarvis; CREATE SCHEMA IF NOT EXISTS amdm; CREATE SCHEMA IF NOT EXISTS pega; CREATE SCHEMA IF NOT EXISTS cmdm; CREATE SCHEMA IF NOT EXISTS sfdc;
USE CATALOG ep_curated;CREATE SCHEMA IF NOT EXISTS jarvis; CREATE SCHEMA IF NOT EXISTS amdm; CREATE SCHEMA IF NOT EXISTS pega; CREATE SCHEMA IF NOT EXISTS cmdm; CREATE SCHEMA IF NOT EXISTS sfdc;
USE CATALOG ep_marts;  CREATE SCHEMA IF NOT EXISTS telecom; CREATE SCHEMA IF NOT EXISTS customer; CREATE SCHEMA IF NOT EXISTS operations;
USE CATALOG ep_ai;     CREATE SCHEMA IF NOT EXISTS knowledge;
```

### sql/gold\_merges.sql (excerpt)

```sql
-- Customer SCD2
MERGE INTO ep_marts.customer.dim_customer_scd2 AS tgt
USING (SELECT prty_id AS customer_id, lower(email) email, sha2(concat_ws('|',prty_id),256) customer_sk, current_timestamp() _ts FROM ep_curated.cmdm.customer_silver) src
ON tgt.customer_id = src.customer_id AND tgt.is_current = true
WHEN MATCHED AND coalesce(tgt.email,'') <> coalesce(src.email,'') THEN UPDATE SET tgt.effective_to = src._ts, tgt.is_current = false
WHEN NOT MATCHED THEN INSERT (customer_sk, customer_id, email, effective_from, effective_to, is_current)
VALUES (src.customer_sk, src.customer_id, src.email, src._ts, TIMESTAMP'9999-12-31 00:00:00', true);
```

---

## 5) DLT & Jobs

### pipelines/dlt/telecom\_dlt.json

```json
{ "name": "telecom_dlt", "continuous": true, "development": false, "clusters": [{"num_workers": 2}] }
```

### pipelines/bundles.yaml (excerpt)

```yaml
bundle: { name: optus-lakehouse }
resources:
  jobs:
    bronze_ingest:
      name: bronze_autoloader
      tasks:
        - task_key: bronze
          notebook_task: { notebook_path: notebooks/01_bronze_autoloader.py, base_parameters: { system: "jarvis" } }
          job_cluster_key: etl_small
      job_clusters:
        - job_cluster_key: etl_small
          new_cluster: { spark_version: 14.3.x-scala2.12, node_type_id: Standard_DS3_v2, num_workers: 2 }
  dlt_pipelines:
    telecom_dlt: { name: telecom_dlt, clusters: [{ num_workers: 2 }], continuous: true }
```

---

## 6) Airflow DAGs (key files)

### orchestration/airflow/dags/domain\_ingest\_daily.py

```python
from airflow import DAG
from airflow.utils.dates import days_ago
from airflow.providers.databricks.operators.databricks import DatabricksRunNowOperator

with DAG("domain_ingest_daily", start_date=days_ago(1), schedule_interval="0 4 * * *", catchup=False) as dag:
    jarvis = DatabricksRunNowOperator(task_id="ingest_jarvis", databricks_conn_id="databricks_default", job_id=1111)
    amdm   = DatabricksRunNowOperator(task_id="ingest_amdm",   databricks_conn_id="databricks_default", job_id=2222)
    pega   = DatabricksRunNowOperator(task_id="ingest_pega",   databricks_conn_id="databricks_default", job_id=3333)
    cmdm   = DatabricksRunNowOperator(task_id="ingest_cmdm",   databricks_conn_id="databricks_default", job_id=4444)
    jarvis >> amdm >> pega >> cmdm
```

### orchestration/airflow/dags/rag\_pipeline.py

```python
from airflow import DAG
from airflow.utils.dates import days_ago
from airflow.providers.databricks.operators.databricks import DatabricksRunNowOperator

with DAG("rag_pipeline", start_date=days_ago(1), schedule_interval=None, catchup=False) as dag:
  ingest_docs = DatabricksRunNowOperator(task_id="ingest_docs", databricks_conn_id="databricks_default", job_id=1401)
  build_cards = DatabricksRunNowOperator(task_id="build_cards", databricks_conn_id="databricks_default", job_id=1402)
  chunk_embed = DatabricksRunNowOperator(task_id="chunk_embed", databricks_conn_id="databricks_default", job_id=1403)
  vs_index = DatabricksRunNowOperator(task_id="vector_index", databricks_conn_id="databricks_default", job_id=1404)
  ingest_docs >> build_cards >> chunk_embed >> vs_index
```

---

## 7) Notebooks (key ones)

### notebooks/01\_bronze\_autoloader.py

```python
from pyspark.sql.functions import input_file_name, current_timestamp
from pyspark.sql.types import *
SYSTEM = dbutils.widgets.get("system") if dbutils.widgets else "sfdc"
CATALOG = "ep_raw"; SCHEMA = SYSTEM; TABLE = f"{SYSTEM}_bronze"; SRC_DIR = f"/mnt/raw/{SYSTEM}/"
spark.sql(f"CREATE TABLE IF NOT EXISTS {CATALOG}.{SCHEMA}.{TABLE} USING DELTA")
df = (spark.readStream.format("cloudFiles").option("cloudFiles.format","csv").load(SRC_DIR)
        .withColumn("source_file", input_file_name()).withColumn("ingest_ts", current_timestamp()))
(df.writeStream.format("delta").option("checkpointLocation", f"/mnt/checkpoints/{SCHEMA}/{TABLE}")
   .outputMode("append").toTable(f"{CATALOG}.{SCHEMA}.{TABLE}"))
```

### notebooks/05c\_rag\_chunk\_and\_embed.py (excerpt)

```python
from pyspark.sql import functions as F, Window
from databricks.sdk import WorkspaceClient
CHUNK_SIZE, OVERLAP = 900, 150
@F.udf('array<string>')
def chunker(t):
    if not t: return []
    step = CHUNK_SIZE-OVERLAP
    return [t[i:i+CHUNK_SIZE] for i in range(0, max(len(t)-1,0), step)]
```

---

## 8) LLM Agents

### llm\_agents/tools/sql\_tool.py

```python
from dataclasses import dataclass
from pyspark.sql import SparkSession
ALLOWED = ["ep_marts","ep_curated"]
@dataclass
class SQLTool:
    name: str = "sql_tool"
    def run(self, query: str) -> str:
        if any(x in query.lower() for x in ["insert","merge","update","delete","drop","alter"]):
            return "Blocked: write ops forbidden"
        if not any(s in query for s in ALLOWED):
            return "Blocked: must reference ep_marts/ep_curated"
        return SparkSession.getActiveSession().sql(query).limit(100).toPandas().to_markdown(index=False)
```

### llm\_agents/serve/router.py

```python
from llm_agents.agents.optus_assistant import Agent
from llm_agents.tools.sql_tool import SQLTool
from llm_agents.tools.retriever_tool import RetrieverTool
agent = Agent(tools={"sql_tool": SQLTool(), "retriever_tool": RetrieverTool()}, safety_filters=[lambda s: s.replace('@','[at]')])

def predict(request: dict) -> dict:
    return {"answer": agent.run(request.get('session_id','na'), request.get('user','unknown'), request['message'])}
```

---

## 9) Quality

### quality/dq\_rules.yaml

```yaml
checks:
  - table: ep_curated.amdm.address_silver
    expectations:
      - expect_column_values_to_not_be_null: { column: amdm_addr_id }
      - expect_column_values_to_match_regex: { column: postcode, regex: "^\\d{4}$" }
```

---

## 10) CI/CD

### .github/workflows/ci.yml

```yaml
name: CI
on: [push, pull_request]
jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with: { python-version: '3.11' }
      - run: pip install -r requirements.txt -r requirements-dev.txt
      - run: pre-commit run --all-files
      - run: pytest -q
      - name: Validate contracts
        run: python tools/validate_contracts.py --schemas domains/**/contracts/*.json
```

### .github/workflows/rag\_eval.yml (optional gate)

```yaml
name: RAG Eval
on:
  pull_request:
    paths: [ 'notebooks/05*', 'llm_agents/**', 'docs/RAG.md' ]
jobs:
  eval:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - run: echo "Run Databricks eval via Bundles or skip if not configured"
```

### azure-pipelines.yml (if Azure DevOps)

```yaml
trigger: none
stages:
- stage: CI
  jobs:
  - job: tests
    pool: { vmImage: 'ubuntu-latest' }
    steps:
      - script: pip install -r requirements.txt -r requirements-dev.txt
      - script: pytest -q
- stage: Deploy_Dev
  dependsOn: CI
  jobs:
  - job: bundles
    steps:
      - script: |
          pip install databricks-sdk databricks-bundles
          databricks bundles deploy -t dev
```

---

## 11) Tools & Tests

### tools/validate\_contracts.py

```python
import json, glob, sys
ok = True
for f in glob.glob(sys.argv[-1]):
    try:
        json.load(open(f))
    except Exception as e:
        print(f"Invalid JSON: {f}: {e}"); ok = False
sys.exit(0 if ok else 1)
```

### tests/unit/test\_transformations.py

```python
def test_placeholder():
    assert 1 == 1
```

---

## 12) What to configure before first push

* Secrets: **Databricks PAT** (for Actions, if used), do **not** commit.
* Paths: mount points under `/mnt/...` and workspace URLs in `bundles.yaml`.
* Job IDs in Airflow variables.
* AAD groups mapping for UC grants.
